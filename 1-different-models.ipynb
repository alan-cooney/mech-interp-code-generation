{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Different Models\n",
    "\n",
    "The first aim here is to find tasks that some models can do but others can't (so that we know something interesting is happening, rather than just e.g. skip trigrams). There are a variety of models to do this with, within [EasyTransformer](https://github.com/neelnanda-io/Easy-Transformer)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install git+https://github.com/neelnanda-io/Easy-Transformer ipython\n",
    "\n",
    "# Clear output\n",
    "from IPython.display import clear_output\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_252/2935899136.py:3: DeprecationWarning: Importing clear_output from IPython.core.display is deprecated since IPython 7.14, please import from IPython display\n",
      "  from IPython.core.display import HTML, clear_output\n"
     ]
    }
   ],
   "source": [
    "from easy_transformer import EasyTransformer\n",
    "import torch\n",
    "from IPython.core.display import HTML, clear_output\n",
    "from typing import List\n",
    "import pandas as pd\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gpt2, gpt2-medium, gpt2-large, gpt2-xl, distilgpt2, facebook/opt-125m, facebook/opt-1.3b, facebook/opt-2.7b, facebook/opt-6.7b, facebook/opt-13b, facebook/opt-30b, facebook/opt-66b, EleutherAI/gpt-neo-125M, EleutherAI/gpt-neo-1.3B, EleutherAI/gpt-neo-2.7B, EleutherAI/gpt-j-6B, EleutherAI/gpt-neox-20b, stanford-crfm/alias-gpt2-small-x21, stanford-crfm/battlestar-gpt2-small-x49, stanford-crfm/caprica-gpt2-small-x81, stanford-crfm/darkmatter-gpt2-small-x343, stanford-crfm/expanse-gpt2-small-x777, stanford-crfm/arwen-gpt2-medium-x21, stanford-crfm/beren-gpt2-medium-x49, stanford-crfm/celebrimbor-gpt2-medium-x81, stanford-crfm/durin-gpt2-medium-x343, stanford-crfm/eowyn-gpt2-medium-x777, EleutherAI/pythia-19m, EleutherAI/pythia-125m, EleutherAI/pythia-350m, EleutherAI/pythia-800m, EleutherAI/pythia-1.3b, EleutherAI/pythia-6.7b, EleutherAI/pythia-13b, EleutherAI/pythia-125m-deduped, EleutherAI/pythia-800m-deduped, EleutherAI/pythia-1.3b-deduped, EleutherAI/pythia-6.7b-deduped, NeelNanda/SoLU_1L_v9_old, NeelNanda/SoLU_2L_v10_old, NeelNanda/SoLU_4L_v11_old, NeelNanda/SoLU_6L_v13_old, NeelNanda/SoLU_8L_v21_old, NeelNanda/SoLU_10L_v22_old, NeelNanda/SoLU_12L_v23_old, NeelNanda/SoLU_1L512W_C4_Code, NeelNanda/SoLU_2L512W_C4_Code, NeelNanda/SoLU_3L512W_C4_Code, NeelNanda/SoLU_4L512W_C4_Code, NeelNanda/SoLU_6L768W_C4_Code, NeelNanda/SoLU_8L1024W_C4_Code, NeelNanda/SoLU_10L1280W_C4_Code, NeelNanda/SoLU_12L1536W_C4_Code, NeelNanda/GELU_1L512W_C4_Code, NeelNanda/GELU_2L512W_C4_Code, NeelNanda/GELU_3L512W_C4_Code, NeelNanda/GELU_4L512W_C4_Code, NeelNanda/Attn_Only_1L512W_C4_Code, NeelNanda/Attn_Only_2L512W_C4_Code, NeelNanda/Attn_Only_3L512W_C4_Code, NeelNanda/Attn_Only_4L512W_C4_Code, NeelNanda/Attn-Only-2L512W-Shortformer-6B-big-lr'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from easy_transformer import loading_from_pretrained\n",
    "\", \".join(loading_from_pretrained.OFFICIAL_MODEL_NAMES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Code completion in language models is pretty sophisticated. For example GitHub Copilot successfully solves problem prompts with solutions, like this:\n",
    "\n",
    "### Python prompt\n",
    "\n",
    "```python\n",
    "# Two Sum\n",
    "\n",
    "# Given an array of integers nums and an integer target, return indices of the two numbers such that they add up to target.\n",
    "\n",
    "# You may assume that each input would have exactly one solution, and you may not use the same element twice.\n",
    "\n",
    "# You can return the answer in any order.\n",
    "\n",
    "class Solution(object):\n",
    "    def twoSum(self, nums, target):\n",
    "```\n",
    "\n",
    "### Solution\n",
    "\n",
    "```python\n",
    "\"\"\"\n",
    ":type nums1: List[int]\n",
    ":type nums2: List[int]\n",
    ":rtype: float\n",
    "\"\"\"\n",
    "nums1.extend(nums2)\n",
    "nums1.sort()\n",
    "if len(nums1)%2 == 0:\n",
    "    return (nums1[len(nums1)//2] + nums1[len(nums1)//2 - 1])/2\n",
    "else:\n",
    "    return nums1[len(nums1)//2]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some interesting observations of learnt skills here include:\n",
    "\n",
    "- Deep stuff: understanding the problem from the comments, and formulating a high level solution.\n",
    "- Keeping track of if/else blocks & brackets\n",
    "- Using previously defined variables (perhaps similar to induction heads)\n",
    "- Knowing variable types (both primitives and library types)\n",
    "- Knowing the methods within classes\n",
    "- Understanding underlying maths (e.g. what modulus and floor divide actually do)\n",
    "- Understanding broadly how a function works (e.g. it typically returns something)\n",
    "- Known line spacing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding interesting problems for different models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tasks in the form [name, prompt, expected_output]\n",
    "tasks: List[List[str]] = [\n",
    "    [\n",
    "        \"Convert types correctly\", \n",
    "        \"\"\"\n",
    "            a = \"The number is\"\n",
    "            b = 12.145\n",
    "            concat = a\"\"\",\n",
    "        \"\"\" + \" \" + str(b)\"\"\"\n",
    "    ]\n",
    "]\n",
    "\n",
    "# Models to use\n",
    "models = [\n",
    "    \"NeelNanda/SoLU_1L512W_C4_Code\", \n",
    "    \"NeelNanda/SoLU_2L512W_C4_Code\", \n",
    "    # \"NeelNanda/SoLU_3L512W_C4_Code\", \n",
    "    # \"NeelNanda/SoLU_4L512W_C4_Code\", \n",
    "    # \"NeelNanda/SoLU_6L768W_C4_Code\", \n",
    "    # \"NeelNanda/SoLU_8L1024W_C4_Code\", \n",
    "    # \"NeelNanda/SoLU_10L1280W_C4_Code\", \n",
    "    # \"NeelNanda/SoLU_12L1536W_C4_Code\", \n",
    "    # \"NeelNanda/GELU_1L512W_C4_Code\", \n",
    "    # \"NeelNanda/GELU_2L512W_C4_Code\", \n",
    "    # \"NeelNanda/GELU_3L512W_C4_Code\", \n",
    "    # \"NeelNanda/GELU_4L512W_C4_Code\", \n",
    "    \"NeelNanda/Attn_Only_1L512W_C4_Code\", \n",
    "    \"NeelNanda/Attn_Only_2L512W_C4_Code\", \n",
    "    # \"NeelNanda/Attn_Only_3L512W_C4_Code\", \n",
    "    # \"NeelNanda/Attn_Only_4L512W_C4_Code\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Task</th>\n",
       "      <th>Prompt</th>\n",
       "      <th>Expected Output</th>\n",
       "      <th>NeelNanda/SoLU_1L512W_C4_Code</th>\n",
       "      <th>NeelNanda/SoLU_2L512W_C4_Code</th>\n",
       "      <th>NeelNanda/Attn_Only_1L512W_C4_Code</th>\n",
       "      <th>NeelNanda/Attn_Only_2L512W_C4_Code</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Convert types correctly</td>\n",
       "      <td>\\n            a = \"The number is\"\\n           ...</td>\n",
       "      <td>+ \" \" + str(b)</td>\n",
       "      <td>.get_at_at(a</td>\n",
       "      <td>.concat(a, b)\\n</td>\n",
       "      <td>\\n            concat = concat\\n            conc</td>\n",
       "      <td>+ \" \" + b + \" \"</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Task                                             Prompt  \\\n",
       "0  Convert types correctly  \\n            a = \"The number is\"\\n           ...   \n",
       "\n",
       "   Expected Output NeelNanda/SoLU_1L512W_C4_Code  \\\n",
       "0   + \" \" + str(b)                  .get_at_at(a   \n",
       "\n",
       "  NeelNanda/SoLU_2L512W_C4_Code  \\\n",
       "0    .concat(a, b)\\n              \n",
       "\n",
       "                NeelNanda/Attn_Only_1L512W_C4_Code  \\\n",
       "0  \\n            concat = concat\\n            conc   \n",
       "\n",
       "  NeelNanda/Attn_Only_2L512W_C4_Code  \n",
       "0                    + \" \" + b + \" \"  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_model_output(\n",
    "    model: EasyTransformer, \n",
    "    prompt: str, \n",
    "    number_new_tokens: int) -> str:\n",
    "    \"\"\"Get the model output for a given prompt\n",
    "\n",
    "    Args:\n",
    "        model (EasyTransformer): Model\n",
    "        prompt (str): Prompt\n",
    "        number_new_tokens (int): Number of output tokens to get (by recursively running the model)\n",
    "\n",
    "    Returns:\n",
    "        str: Output tokens as a concatenated string\n",
    "    \"\"\"\n",
    "    next_tokens: List[str] = []\n",
    "    \n",
    "    for i in range(number_new_tokens):\n",
    "        logits = model(prompt + \"\".join(next_tokens))\n",
    "        predictions = torch.argmax(logits, 2)\n",
    "        prediction = int(predictions[0][-1].item())\n",
    "        next_token = model.tokenizer.decode(prediction)\n",
    "        next_tokens.append(next_token)\n",
    "        \n",
    "    return \"\".join(next_tokens)\n",
    "\n",
    "tasks_with_model_outputs = copy.deepcopy(tasks)\n",
    "\n",
    "for model_idx, model_name in enumerate(models):\n",
    "    # Clear PyTorch GPU memory\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    # Load up the model\n",
    "    model = EasyTransformer.from_pretrained(model_name)\n",
    "    \n",
    "    # Loop through tasks\n",
    "    for task_idx, task in enumerate(tasks):\n",
    "        \n",
    "        # Destructure the tasks\n",
    "        print(task)\n",
    "        print(len(task))\n",
    "\n",
    "        [name, prompt, expected_output] = task\n",
    "    \n",
    "        # Get the number of tokens in the expected output\n",
    "        expected_tokens = model.tokenizer.encode(expected_output)\n",
    "        expected_tokens_length = len(expected_tokens)\n",
    "        \n",
    "        # Get the model output\n",
    "        model_output = get_model_output(model, prompt, expected_tokens_length)\n",
    "        tasks_with_model_outputs[task_idx].append(model_output)\n",
    "        \n",
    "\n",
    "# Display the results\n",
    "results = pd.DataFrame(tasks_with_model_outputs, columns=[\"Task\", \"Prompt\", \"Expected Output\", *models])\n",
    "clear_output()\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### save\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<br/><br/><br/>def twoSum(self, num"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the model\n",
    "model_name = \"NeelNanda/SoLU_12L1536W_C4_Code\"\n",
    "model = EasyTransformer.from_pretrained(model_name)\n",
    "\n",
    "# Complete a LeetCode test\n",
    "prompt = \"\"\"# Two Sum\n",
    "\n",
    "# Given an array of integers nums and an integer target, return indices of the two numbers such that they add up to target.\n",
    "\n",
    "# You may assume that each input would have exactly one solution, and you may not use the same element twice.\n",
    "\n",
    "# You can return the answer in any order.\n",
    "\n",
    "class Solution(object):\n",
    "    def twoSum(self, nums, target):\"\"\"\n",
    "\n",
    "next_tokens = []\n",
    "for i in range(10):\n",
    "    logits = model(prompt + \"\".join(next_tokens))\n",
    "    predictions = torch.argmax(logits, 2)\n",
    "    prediction = int(predictions[0][-1].item())\n",
    "    next_token = model.tokenizer.decode(prediction)\n",
    "    next_tokens.append(next_token)\n",
    "    \n",
    "HTML(\"\".join(next_tokens).replace(\"\\n\", \"<br/>\"))\n",
    "\n",
    "\n",
    "        # \"\"\"\n",
    "        # :type nums1: List[int]\n",
    "        # :type nums2: List[int]\n",
    "        # :rtype: float\n",
    "        # \"\"\"\n",
    "        # nums1.extend(nums2)\n",
    "        # nums1.sort()\n",
    "        # if len(nums1)%2 == 0:\n",
    "        #     return (nums1[len(nums1)//2] + nums1[len(nums1)//2 - 1])/2\n",
    "        # else:\n",
    "        #     return nums1[len(nums1)//2]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "949777d72b0d2535278d3dc13498b2535136f6dfe0678499012e853ee9abcab1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
