{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Different Models\n",
    "\n",
    "The first aim here is to find tasks that some models can do but others can't (so that we know something interesting is happening, rather than just e.g. skip trigrams). There are a variety of models to do this with, within [EasyTransformer](https://github.com/neelnanda-io/Easy-Transformer)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gpt2, gpt2-medium, gpt2-large, gpt2-xl, distilgpt2, facebook/opt-125m, facebook/opt-1.3b, facebook/opt-2.7b, facebook/opt-6.7b, facebook/opt-13b, facebook/opt-30b, facebook/opt-66b, EleutherAI/gpt-neo-125M, EleutherAI/gpt-neo-1.3B, EleutherAI/gpt-neo-2.7B, EleutherAI/gpt-j-6B, EleutherAI/gpt-neox-20b, stanford-crfm/alias-gpt2-small-x21, stanford-crfm/battlestar-gpt2-small-x49, stanford-crfm/caprica-gpt2-small-x81, stanford-crfm/darkmatter-gpt2-small-x343, stanford-crfm/expanse-gpt2-small-x777, stanford-crfm/arwen-gpt2-medium-x21, stanford-crfm/beren-gpt2-medium-x49, stanford-crfm/celebrimbor-gpt2-medium-x81, stanford-crfm/durin-gpt2-medium-x343, stanford-crfm/eowyn-gpt2-medium-x777, EleutherAI/pythia-19m, EleutherAI/pythia-125m, EleutherAI/pythia-350m, EleutherAI/pythia-800m, EleutherAI/pythia-1.3b, EleutherAI/pythia-6.7b, EleutherAI/pythia-13b, EleutherAI/pythia-125m-deduped, EleutherAI/pythia-800m-deduped, EleutherAI/pythia-1.3b-deduped, EleutherAI/pythia-6.7b-deduped, NeelNanda/SoLU_1L_v9_old, NeelNanda/SoLU_2L_v10_old, NeelNanda/SoLU_4L_v11_old, NeelNanda/SoLU_6L_v13_old, NeelNanda/SoLU_8L_v21_old, NeelNanda/SoLU_10L_v22_old, NeelNanda/SoLU_12L_v23_old, NeelNanda/SoLU_1L512W_C4_Code, NeelNanda/SoLU_2L512W_C4_Code, NeelNanda/SoLU_3L512W_C4_Code, NeelNanda/SoLU_4L512W_C4_Code, NeelNanda/SoLU_6L768W_C4_Code, NeelNanda/SoLU_8L1024W_C4_Code, NeelNanda/SoLU_10L1280W_C4_Code, NeelNanda/SoLU_12L1536W_C4_Code, NeelNanda/GELU_1L512W_C4_Code, NeelNanda/GELU_2L512W_C4_Code, NeelNanda/GELU_3L512W_C4_Code, NeelNanda/GELU_4L512W_C4_Code, NeelNanda/Attn_Only_1L512W_C4_Code, NeelNanda/Attn_Only_2L512W_C4_Code, NeelNanda/Attn_Only_3L512W_C4_Code, NeelNanda/Attn_Only_4L512W_C4_Code, NeelNanda/Attn-Only-2L512W-Shortformer-6B-big-lr'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from easy_transformer import loading_from_pretrained\n",
    "\", \".join(loading_from_pretrained.OFFICIAL_MODEL_NAMES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "from easy_transformer import EasyTransformer\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Code completion in language models is pretty sophisticated. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# Load the model\n",
    "model_name = \"gpt2-medium\" \n",
    "model = EasyTransformer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "!ls -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# Complete a LeetCode test\n",
    "prompt = \"\"\"# Two Sum\n",
    "\n",
    "# Given an array of integers nums and an integer target, return indices of the two numbers such that they add up to target.\n",
    "\n",
    "# You may assume that each input would have exactly one solution, and you may not use the same element twice.\n",
    "\n",
    "# You can return the answer in any order.\n",
    "\n",
    "class Solution(object):\n",
    "    def twoSum(self, nums, target):\"\"\"\n",
    "\n",
    "next_tokens = []\n",
    "for i in range(10):\n",
    "    logits = model(prompt + \"\".join(next_tokens))\n",
    "    predictions = torch.argmax(logits, 2)\n",
    "    prediction = int(predictions[0][-1].item())\n",
    "    next_token = model.tokenizer.decode(prediction)\n",
    "    next_tokens.append(next_token)\n",
    "    \n",
    "\"\".join(next_tokens).replace(\"\\n\", \"<br/>\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "949777d72b0d2535278d3dc13498b2535136f6dfe0678499012e853ee9abcab1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
