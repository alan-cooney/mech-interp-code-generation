{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Exploratory Analysis"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "2sroRK0WZFPQ"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "f5BGufuBZFPQ",
        "outputId": "361b9fa2-65f7-4f3b-c36f-8633dea29e97"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running as a Jupyter notebook - intended for development only!\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_15025/3477142988.py:7: DeprecationWarning: `magic(...)` is deprecated since IPython 0.13 (warning added in 8.1), use run_line_magic(magic_name, parameter_s).\n",
            "  ipython.magic(\"load_ext autoreload\")\n",
            "/tmp/ipykernel_15025/3477142988.py:8: DeprecationWarning: `magic(...)` is deprecated since IPython 0.13 (warning added in 8.1), use run_line_magic(magic_name, parameter_s).\n",
            "  ipython.magic(\"autoreload 2\")\n"
          ]
        }
      ],
      "source": [
        "IN_COLAB = False\n",
        "print(\"Running as a Jupyter notebook - intended for development only!\")\n",
        "from IPython import get_ipython\n",
        "\n",
        "ipython = get_ipython()\n",
        "# Code to automatically update the EasyTransformer code as its edited without restarting the kernel\n",
        "ipython.magic(\"load_ext autoreload\")\n",
        "ipython.magic(\"autoreload 2\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "CdhBe7EiZFPS"
      },
      "outputs": [],
      "source": [
        "# Import stuff\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import ast\n",
        "import einops\n",
        "from fancy_einsum import einsum\n",
        "import tqdm.notebook as tqdm\n",
        "import random\n",
        "from pathlib import Path\n",
        "import plotly.express as px\n",
        "from torch.utils.data import DataLoader\n",
        "import pandas as pd\n",
        "from IPython.display import clear_output\n",
        "\n",
        "from torchtyping import TensorType as TT\n",
        "from typing import List, Union, Optional\n",
        "from functools import partial\n",
        "import copy\n",
        "\n",
        "import itertools\n",
        "from transformers import AutoModelForCausalLM, AutoConfig, AutoTokenizer\n",
        "import dataclasses\n",
        "import datasets\n",
        "from IPython.display import HTML\n",
        "from torchtyping import TensorType"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Xg5PMCIfZFPS"
      },
      "outputs": [],
      "source": [
        "import circuitsvis\n",
        "from circuitsvis import attention\n",
        "\n",
        "import easy_transformer\n",
        "import easy_transformer.utils as utils\n",
        "from easy_transformer.hook_points import (\n",
        "    HookedRootModule,\n",
        "    HookPoint,\n",
        ")  # Hooking utilities\n",
        "from easy_transformer import EasyTransformer, EasyTransformerConfig, FactoredMatrix, ActivationCache"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_y0vPDdWZFPS"
      },
      "source": [
        "We turn automatic differentiation off, to save GPU memory, as this notebook focuses on model inference not model training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "QPaNVqXuZFPT",
        "outputId": "0c608e70-717a-4897-92f8-f15f39ff0e22"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<torch.autograd.grad_mode.set_grad_enabled at 0x7f2224639820>"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "torch.set_grad_enabled(False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PifPQguUZFPT"
      },
      "source": [
        "Plotting helper functions:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "-zmMtKY6ZFPT"
      },
      "outputs": [],
      "source": [
        "def imshow(tensor, renderer=None, **kwargs):\n",
        "    px.imshow(utils.to_numpy(tensor), color_continuous_midpoint=0.0, color_continuous_scale=\"RdBu\", **kwargs).show(renderer)\n",
        "\n",
        "def line(tensor, renderer=None, **kwargs):\n",
        "    px.line(y=utils.to_numpy(tensor), **kwargs).show(renderer)\n",
        "\n",
        "def scatter(x, y, xaxis=\"\", yaxis=\"\", caxis=\"\", renderer=None, **kwargs):\n",
        "    x = utils.to_numpy(x)\n",
        "    y = utils.to_numpy(y)\n",
        "    px.scatter(y=y, x=x, labels={\"x\":xaxis, \"y\":yaxis, \"color\":caxis}, **kwargs).show(renderer)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "bfJecbZHZFPU"
      },
      "source": [
        "## Prompts to analyse"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Model setup"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "fzuAxPL5ZFPU"
      },
      "source": [
        "We're mainly interested in the SoLU model, but can also use the attn model for\n",
        "comparison."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Ku-NkdvNZFPU",
        "outputId": "347df070-d79f-4995-9996-5314795617f5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading model: NeelNanda/SoLU_1L512W_C4_Code\n",
            "Moving model to device:  cuda\n",
            "Finished loading pretrained model NeelNanda/SoLU_1L512W_C4_Code into EasyTransformer!\n",
            "Loading model: NeelNanda/Attn_Only_1L512W_C4_Code\n",
            "Moving model to device:  cuda\n",
            "Finished loading pretrained model NeelNanda/Attn_Only_1L512W_C4_Code into EasyTransformer!\n"
          ]
        }
      ],
      "source": [
        "model = EasyTransformer.from_pretrained(\n",
        "    \"NeelNanda/SoLU_1L512W_C4_Code\",\n",
        "    center_unembed=True,\n",
        "    center_writing_weights=True,\n",
        "    fold_ln=True,\n",
        "    refactor_factored_attn_matrices=True,\n",
        ")\n",
        "\n",
        "attn_model = EasyTransformer.from_pretrained(\n",
        "    \"NeelNanda/Attn_Only_1L512W_C4_Code\",\n",
        "    center_unembed=True,\n",
        "    center_writing_weights=True,\n",
        "    fold_ln=True,\n",
        "    refactor_factored_attn_matrices=True,\n",
        "    )"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Verify the model gets it right"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "TTBjKYGlZFPU",
        "outputId": "01cfda82-fa16-4245-bf21-2c158ed8ec7f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tokenized prompt: ['<|BOS|>', 'There', ' was', ' a', ' notable', ' person', ' called', \" '\", 'Stephen', ' Haw']\n",
            "Tokenized answer: ['king']\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Performance on answer token:\n",
              "<span style=\"font-weight: bold\">Rank: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"font-weight: bold\">        Logit: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">20.73</span><span style=\"font-weight: bold\"> Prob: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">56.08</span><span style=\"font-weight: bold\">% Token: |king|</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "Performance on answer token:\n",
              "\u001b[1mRank: \u001b[0m\u001b[1;36m0\u001b[0m\u001b[1m        Logit: \u001b[0m\u001b[1;36m20.73\u001b[0m\u001b[1m Prob: \u001b[0m\u001b[1;36m56.08\u001b[0m\u001b[1m% Token: |king|\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Top 0th token. Logit: 20.73 Prob: 56.08% Token: |king|\n",
            "Top 1th token. Logit: 19.26 Prob: 12.92% Token: |th|\n",
            "Top 2th token. Logit: 18.47 Prob:  5.86% Token: |kes|\n",
            "Top 3th token. Logit: 18.35 Prob:  5.22% Token: |ker|\n",
            "Top 4th token. Logit: 18.21 Prob:  4.54% Token: |ley|\n",
            "Top 5th token. Logit: 17.57 Prob:  2.39% Token: |orth|\n",
            "Top 6th token. Logit: 17.53 Prob:  2.29% Token: |ken|\n",
            "Top 7th token. Logit: 17.05 Prob:  1.41% Token: |key|\n",
            "Top 8th token. Logit: 16.96 Prob:  1.30% Token: |ke|\n",
            "Top 9th token. Logit: 16.68 Prob:  0.98% Token: |son|\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Ranks of the answer tokens:</span> <span style=\"font-weight: bold\">[(</span><span style=\"color: #008000; text-decoration-color: #008000\">'king'</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"font-weight: bold\">)]</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1mRanks of the answer tokens:\u001b[0m \u001b[1m[\u001b[0m\u001b[1m(\u001b[0m\u001b[32m'king'\u001b[0m, \u001b[1;36m0\u001b[0m\u001b[1m)\u001b[0m\u001b[1m]\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Using a standard format\n",
        "example_prompt = \"There was a notable person called 'Stephen Haw\"\n",
        "example_answer = \"king\"\n",
        "prompt = example_prompt + example_answer\n",
        "utils.test_prompt(example_prompt, example_answer, model=model, prepend_bos=True, prepend_space_to_answer=False)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Generate a bunch of prompts and answers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "data = pd.read_csv(\"data/sample_names.csv\")\n",
        "\n",
        "prompts = []\n",
        "answers = []\n",
        "model_results = []\n",
        "\n",
        "# Iterate over pandas rows\n",
        "for index, row in data.iterrows():\n",
        "    tokens = ast.literal_eval(row[\"tokenized\"])\n",
        "    bigram = row[\"bigrams\"]\n",
        "    \n",
        "    prompt_str = model.tokenizer.decode(tokens[0:2])\n",
        "    answer_str = model.tokenizer.decode(tokens[2])\n",
        "    bigram_str = model.tokenizer.decode(bigram)\n",
        "    \n",
        "    print(model(prompt_str).shape)\n",
        "    break\n",
        "    res = torch.argmax(model(prompt_str), dim=-1).item()\n",
        "    model_results.append(res)\n",
        "    \n",
        "    prompts.append(\"There was a notable person called '\"+ prompt_str)\n",
        "    answers.append((answer_str, bigram_str))\n",
        "\n",
        "# clear_output(wait=True)\n",
        " \n",
        "# pd.DataFrame({\"prompt\":prompts, \"answer\":answers, \"model_res\": model_results}).head(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P7ehojg9ZFPU",
        "outputId": "a85d5b30-144b-4785-9246-6964e1d73644"
      },
      "outputs": [],
      "source": [
        "# Note prompts need to be the same length (in tokens) for activation patching\n",
        "# (whereas they don't appear to need this for the other methods)\n",
        "# TODO: Think about making strings by hand (by token numbers)\n",
        "# prompt_format = [\n",
        "#     # \".he{}odes .he{}odes .he\",\n",
        "#     # TODO: Consider replacing self/odes stuff with a lot of random things\n",
        "#     \"self.he{}odes self.he{}odes self.he\",\n",
        "#     \"this.he{}odes this.he{}odes this.he\", # TODO: re-run with this fixed\n",
        "#     # \".he{} .he{} .he\",\n",
        "# ]\n",
        "\n",
        "# names = [\n",
        "#     (\"xc\", \"xa\"),\n",
        "#     (\"xc\", \"xa\"),\n",
        "#     # (\"xc\", \"xa\"),\n",
        "# ]\n",
        "\n",
        "# # List of prompts\n",
        "# prompts = []\n",
        "# # List of answers, in the format (correct, incorrect)\n",
        "# answers = []\n",
        "\n",
        "# for i in range(len(prompt_format)):\n",
        "#     for j in range(2):\n",
        "#         correct = names[i][j]\n",
        "#         incorrect = names[i][1 - j]\n",
        "#         prompts.append(prompt_format[i].format(correct, correct))\n",
        "#         answers.append((correct, incorrect))\n",
        "\n",
        "# print(prompts)\n",
        "# print(answers)\n",
        "\n",
        "for prompt in prompts:\n",
        "    assert len(model.to_tokens(prompt)) == len(model.to_tokens(prompts[0]))\n",
        "\n",
        "# Answer tokens = [(correct_token, incorrect_token)... for each prompt]\n",
        "answer_tokens = [(model.to_single_token(answer[0]), model.to_single_token(answer[1])) for answer in answers]\n",
        "answer_tokens = torch.tensor(answer_tokens).cuda()\n",
        "print(answer_tokens)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "jgBbILmZZFPV"
      },
      "source": [
        "### Run with the cache"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uqRgVap9ZFPV"
      },
      "outputs": [],
      "source": [
        "# Get the tokens for each prompt [ prompt x tokens ]\n",
        "# Note shorter prompts will be padded\n",
        "tokens = model.to_tokens(prompts, prepend_bos=True)\n",
        "tokens = tokens.cuda()\n",
        "print(tokens.shape)\n",
        "\n",
        "# Run the model and cache all activations\n",
        "original_logits, cache = model.run_with_cache(tokens)\n",
        "\n",
        "# Logits [ prompt x token x logits]\n",
        "original_logits.shape"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Setup logit diff function"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "rqg6ALzJZFPV"
      },
      "source": [
        "We'll later be evaluating how model performance differs upon performing various\n",
        "interventions, so it's useful to have a metric to measure model performance. Our\n",
        "metric here will be the **logit difference**, the difference in logit between\n",
        "the correct and incorrect answers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VwRiUEcCZFPV",
        "outputId": "a0ce46f3-7f3b-4c8d-a0f5-04b3f0f279d0"
      },
      "outputs": [],
      "source": [
        "def logits_to_ave_logit_diff(logits, answer_tokens, per_prompt=False):\n",
        "    # Only the final logits are relevant for the answer\n",
        "    final_logits = logits[:, -1, :]\n",
        "    answer_logits = final_logits.gather(dim=-1, index=answer_tokens)\n",
        "    answer_logit_diff = answer_logits[:, 0] - answer_logits[:, 1]\n",
        "    if per_prompt:\n",
        "        return answer_logit_diff\n",
        "    else:\n",
        "        return answer_logit_diff.mean()\n",
        "\n",
        "print(\"Per prompt logit difference:\", logits_to_ave_logit_diff(original_logits, answer_tokens, per_prompt=True))\n",
        "original_average_logit_diff = logits_to_ave_logit_diff(original_logits, answer_tokens)\n",
        "print(\"Average logit difference:\", logits_to_ave_logit_diff(original_logits, answer_tokens).item())"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "I9Qx9VcnZFPV"
      },
      "source": [
        "$e^{4.3}\\approx 75\\times$ higher probability on the correct answer. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Brainstorm what's going on"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "L1LAV94RZFPV"
      },
      "source": [
        "In general I think that commonly \"he\" will be followed by \"xa\" for e.g.\n",
        "\"hexadecimal\", so this is probably the default bigram solution. A single\n",
        "attention layer can't do better than this as it can't do skip trigrams (i.e. he\n",
        "-> he -> xc). However adding an MLP can potentially do this, if for some reason\n",
        "the xc token is attended to by one head (and then the MLP layer can adjudicate\n",
        "between this and other potential next tokens).\n",
        "\n",
        "One way to have this is a \"repeated bigrams head\" where destination tokens such\n",
        "as \"he\" will attend to source tokens such as \"xa\" (e.g. for hexadecimal) and\n",
        "\"xc\" (e.g. for \"hexcodes\"). If this is the case, we'd expect similar things\n",
        "where bigram statistics give two close output tokens."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "lMhDY1HbZFPV"
      },
      "source": [
        "## Direct Logit Attribution"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "qDEJkk6_ZFPV"
      },
      "source": [
        "We'll compare the correct and incorrect output tokens using logit difference directions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z_NGTYVuZFPW",
        "outputId": "7d388dc4-81d9-4a63-c6f3-12f44235e344"
      },
      "outputs": [],
      "source": [
        "# Answer residual directions [ prompt x correct/incorrect (i.e. 2) x d_model ]\n",
        "answer_residual_directions = model.tokens_to_residual_directions(answer_tokens)\n",
        "print(\"Answer residual directions shape:\", answer_residual_directions.shape)\n",
        "\n",
        "# Logit difference directions [ prompt x d_model ]\n",
        "logit_diff_directions = answer_residual_directions[:, 0] - answer_residual_directions[:, 1]\n",
        "print(\"Logit difference directions shape:\", logit_diff_directions.shape)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "e5W01YmlZFPW"
      },
      "source": [
        "To verify that this works, we can apply this to the final residual stream for our cached prompts (after applying LayerNorm scaling) and verify that we get the same answer. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FAs4UrOdZFPW",
        "outputId": "c677db6d-70dc-4989-f790-97023820227e"
      },
      "outputs": [],
      "source": [
        "# Scope to function to avoid conflicts\n",
        "def verify_resid_directions():\n",
        "    # cache syntax - resid_post is the residual stream at the end of the layer, -1 gets the final layer. The general syntax is [activation_name, layer_index, sub_layer_type]. \n",
        "    final_residual_stream = cache[\"resid_post\", -1]\n",
        "    # print(dict(cache).keys())\n",
        "    print(\"Final residual stream shape:\", final_residual_stream.shape)\n",
        "    final_token_residual_stream = final_residual_stream[:, -1, :]\n",
        "    # Apply LayerNorm scaling\n",
        "    # pos_slice is the subset of the positions we take - here the final token of each prompt\n",
        "    scaled_final_token_residual_stream = cache.apply_ln_to_stack(final_token_residual_stream, layer = -1, pos_slice=-1)\n",
        "\n",
        "    average_logit_diff = einsum(\"batch d_model, batch d_model -> \", scaled_final_token_residual_stream, logit_diff_directions)/len(prompts)\n",
        "    print(\"Calculated average logit diff:\", average_logit_diff.item())\n",
        "    print(\"Original logit difference:\",original_average_logit_diff.item())\n",
        "    \n",
        "verify_resid_directions()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "hW8zQQvXZFPW"
      },
      "source": [
        "### Logit Lens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jb1ShnzYZFPW"
      },
      "outputs": [],
      "source": [
        "def residual_stack_to_logit_diff(\n",
        "    residual_stack: TT[\"components\", \"prompt\", \"d_model\"], \n",
        "    cache: ActivationCache,\n",
        "    logit_difference_directions: TT[\"prompt\", \"d_model\"],\n",
        "    prompts_list: List[str]\n",
        "    ) -> TT[\"components\"]:\n",
        "    \"\"\"Per component (layer) residual values -> per component logit difference\n",
        "\n",
        "    Args:\n",
        "        residual_stack: Per layer residual (e.g. the residuals right after each layer)\n",
        "        cache: Cached activations from a model run\n",
        "        logit_difference_directions: Logit difference directions [ prompt x d_model ]\n",
        "        prompts_list: List of prompts\n",
        "\n",
        "    Returns:\n",
        "        Logit difference per layer\n",
        "    \"\"\"\n",
        "    # Get the residual stack (after each component/layer), scaled by the\n",
        "    # LayerNorm after each layer.\n",
        "    scaled_residual_stack: TT[\"components\", \"prompt\", \"d_model\"] = cache.apply_ln_to_stack(\n",
        "        residual_stack, \n",
        "        layer = -1, # Layer we're taking the input to (-1 = unembed)\n",
        "        pos_slice=-1 # Position we're taking the input to (-1 = final token) \n",
        "        )\n",
        "    \n",
        "    sum_logit_diff_per_component: TT[\"components\"] = einsum(\n",
        "        \"components prompt d_model, prompt d_model -> components\", \n",
        "        scaled_residual_stack, \n",
        "        logit_difference_directions\n",
        "        )\n",
        "    \n",
        "    # Divide by number of prompts. TODO: Why????\n",
        "    return sum_logit_diff_per_component/len(prompts_list)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Czrg0_0SZFPW",
        "outputId": "dfb80fd9-d228-42c1-edab-e4be23738c09"
      },
      "outputs": [],
      "source": [
        "# Get the (accumulated) residuals after each layer\n",
        "per_layer_residual, labels = cache.decompose_resid(layer=-1, pos_slice=-1, return_labels=True)\n",
        "\n",
        "# Get the logit difference per layer\n",
        "per_layer_logit_diffs = residual_stack_to_logit_diff(per_layer_residual, cache, logit_diff_directions, prompts)\n",
        "\n",
        "line(per_layer_logit_diffs, hover_name=labels, title=\"Logit Difference From Each Layer\")\n",
        "\n",
        "# TODO: Replace first occurrence with he by something else -> use as baseline\n",
        "# (rather than xc/xa). Also try xa in other parts of the text."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This shows broadly that the attention layer helps (which we'd expect if a head\n",
        "is attending to common pairings), and then the MLP layer gets us over the line.\n",
        "\n",
        "__TODO: A question I have is__ does the attention layer alone get it right (i.e. in\n",
        "this model is the MLP layer needed). It's possible that the architecture, in\n",
        "allowing for adjudication, actually also just gets things right without it\n",
        "(whereas without the MLP layer it's not really incentivised to setup a \"common\n",
        "bigrams head\")."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5_PzTg9fZFPX"
      },
      "source": [
        "## Head Attribution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LH9Dr-IhZFPX",
        "outputId": "b55274c5-9a20-4463-e1ee-7af710991d60"
      },
      "outputs": [],
      "source": [
        "# Get a stack of all the head contributions to the residual stream\n",
        "per_head_residual, labels = cache.stack_head_results(layer=-1, pos_slice=-1, return_labels=True)\n",
        "\n",
        "per_head_logit_diffs = residual_stack_to_logit_diff(per_head_residual, cache, logit_diff_directions, prompts)\n",
        "per_head_logit_diffs = einops.rearrange(per_head_logit_diffs, \"(layer head_index) -> layer head_index\", layer=model.cfg.n_layers, head_index=model.cfg.n_heads)\n",
        "imshow(per_head_logit_diffs, labels={\"x\":\"Head\", \"y\":\"Layer\"}, title=\"Logit Difference From Each Head\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "fw_O6JecZFPX"
      },
      "source": [
        "### Attention Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q7ZX-zadZFPX"
      },
      "outputs": [],
      "source": [
        "def visualize_attention_patterns(\n",
        "    heads: Union[List[int], int, TT[\"heads\"]], \n",
        "    model_number_heads: int,\n",
        "    batch_item_index: int,\n",
        "    local_cache: ActivationCache, \n",
        "    local_tokens: torch.Tensor,\n",
        "    title: str=\"\"):\n",
        "    \n",
        "    # Convert list of head numbers to numpy array\n",
        "    if isinstance(heads, int):\n",
        "        heads = [heads]\n",
        "    if isinstance(heads, list) or isinstance(heads, torch.Tensor):\n",
        "        heads = utils.to_numpy(heads)\n",
        "    \n",
        "    # Labels/patterns\n",
        "    labels = []\n",
        "    patterns = []\n",
        "    \n",
        "    for head in heads:\n",
        "        layer = head // model_number_heads\n",
        "        head_index = head % model_number_heads\n",
        "        # Get the attention patterns for the head\n",
        "        # Attention patterns have shape [batch, head_index, query_pos, key_pos]\n",
        "        patterns.append(local_cache[\"attn\", layer][batch_item_index, head_index])\n",
        "        labels.append(f\"L{layer}H{head_index}\")\n",
        "        \n",
        "    str_tokens = model.to_str_tokens(local_tokens)\n",
        "    patterns = torch.stack(patterns, dim=-1)\n",
        "    patterns = einops.rearrange(patterns, \"query_pos key_pos heads -> heads query_pos key_pos\")\n",
        "    \n",
        "    # Plot\n",
        "    attention_vis = attention.attention_heads(attention=patterns, tokens=str_tokens, attention_head_names=labels)\n",
        "    \n",
        "    # Display\n",
        "    display(HTML(f\"<h2>{title}</h2>\"))\n",
        "    display(attention_vis)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ni3gLYFwZFPX",
        "outputId": "16acb083-9cea-4f7b-b782-200b3bf8076e"
      },
      "outputs": [],
      "source": [
        "# Get the top k head numbers (by highest positive logit difference)\n",
        "top_k = 4\n",
        "top_positive_logit_attr_heads = torch.topk(per_head_logit_diffs.flatten(), k=top_k).indices\n",
        "print(top_positive_logit_attr_heads)\n",
        "\n",
        "# Prompt to visualize\n",
        "prompt_number = 0\n",
        "\n",
        "# Visualize the attention patterns for them\n",
        "visualize_attention_patterns(\n",
        "    top_positive_logit_attr_heads,\n",
        "    model_number_heads=model.cfg.n_heads,\n",
        "    batch_item_index=prompt_number,\n",
        "    local_cache=cache, \n",
        "    local_tokens=tokens[prompt_number],\n",
        "    title=f\"Top {top_k} Positive Logit Attribution Heads - Prompt {prompt_number}\"\n",
        "    )\n",
        "\n",
        "\n",
        "top_negative_logit_attr_heads = torch.topk(-per_head_logit_diffs.flatten(), k=top_k).indices\n",
        "visualize_attention_patterns(\n",
        "    top_negative_logit_attr_heads,\n",
        "    model_number_heads=model.cfg.n_heads,\n",
        "    batch_item_index=prompt_number,\n",
        "    local_cache=cache, \n",
        "    local_tokens=tokens[prompt_number],    \n",
        "    title=f\"Top {top_k} Negative Logit Attribution Heads\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Kind of interesting that H6 appears to be doing bigram style stuff. E.g. `self`\n",
        "(dest) -> `.` (src), he -> xc, xc -> odes, dot-> xc\n",
        "\n",
        "Also showing with the counter-factual (\"xa\" not \"xc\" as the correct answer), to\n",
        "check it's still doing some sort of fake induction:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Prompt to visualize\n",
        "prompt_number = 1\n",
        "\n",
        "# Visualize the attention patterns for them\n",
        "visualize_attention_patterns(\n",
        "    top_positive_logit_attr_heads,\n",
        "    model_number_heads=model.cfg.n_heads,\n",
        "    batch_item_index=prompt_number,\n",
        "    local_cache=cache, \n",
        "    local_tokens=tokens[prompt_number],\n",
        "    title=f\"Top {top_k} Positive Logit Attribution Heads - Prompt {prompt_number}\"\n",
        "    )"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Seems like H6 is a repeated bigrams head (responds to both `xa` and `xc`)."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "vRfiPMQKZFPX"
      },
      "source": [
        "## Activation Patching"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "JiYnCqemZFPX"
      },
      "source": [
        "### Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MYxHooEeZFPY",
        "outputId": "fa7f8686-eb82-4a1f-b4bd-9f66633f016e"
      },
      "outputs": [],
      "source": [
        "# Alternative the prompt order to incorrect, correct... so that the answers are\n",
        "# now all incorrect rather than all correct.\n",
        "corrupted_prompts = []\n",
        "for i in range(0, len(prompts), 2):\n",
        "    corrupted_prompts.append(prompts[i+1])\n",
        "    corrupted_prompts.append(prompts[i])\n",
        "print(corrupted_prompts)\n",
        "\n",
        "# Run the model with the corrupted tokens\n",
        "corrupted_tokens = model.to_tokens(corrupted_prompts, prepend_bos=True)\n",
        "corrupted_logits, corrupted_cache = model.run_with_cache(corrupted_tokens, return_type=\"logits\")\n",
        "\n",
        "# Get the \"corrupted\" logit differences (which should be the opposite of the non-corrupted ones)\n",
        "corrupted_average_logit_diff = logits_to_ave_logit_diff(corrupted_logits, answer_tokens)\n",
        "print(\"Corrupted Average Logit Diff\", corrupted_average_logit_diff)\n",
        "print(\"Clean Average Logit Diff\", original_average_logit_diff)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YgVAf5RaZFPY",
        "outputId": "6753890f-e3ed-462a-8e44-fa058a55638d"
      },
      "outputs": [],
      "source": [
        "model.to_string(corrupted_tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-7IuoUfxZFPY"
      },
      "source": [
        "We now intervene on the corrupted run and patch in the clean residual stream at a specific layer and position.\n",
        "\n",
        "We do the intervention using EasyTransformer's `HookPoint` feature. We can design a hook function that takes in a specific activation and returns an edited copy, and temporarily add it in with `model.run_with_hooks`. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bid3W7TZZFPY"
      },
      "outputs": [],
      "source": [
        "def patch_residual_component(\n",
        "    corrupted_residual_component: TT[\"batch\", \"pos\", \"d_model\"],\n",
        "    hook, \n",
        "    pos, \n",
        "    clean_cache\n",
        "    ):\n",
        "    \"\"\"Patch a corrupted residual component, with the clean residual component\n",
        "    \n",
        "    For a specific residual component, for all prompts, in a set position, all the residual values\n",
        "    \"\"\"\n",
        "    corrupted_residual_component[:, pos, :] = clean_cache[hook.name][:, pos, :]\n",
        "    return corrupted_residual_component\n",
        "\n",
        "def normalize_patched_logit_diff(patched_logit_diff):\n",
        "    \"\"\"Normalize the patched logit difference (to be between -1 and 1)\n",
        "    \n",
        "    Subtract corrupted logit diff to measure the improvement, divide by the\n",
        "    total improvement from clean to corrupted to normalize.\n",
        "    \n",
        "    0 means zero change, negative means actively made worse, 1 means totally\n",
        "    recovered clean performance, >1 means actively *improved* on clean\n",
        "    performance.\n",
        "    \"\"\"\n",
        "    return (patched_logit_diff - corrupted_average_logit_diff)/(original_average_logit_diff - corrupted_average_logit_diff)\n",
        "\n",
        "# Patched logit difference for each attention layer (just 1) and position (14)\n",
        "# Initialize:\n",
        "patched_residual_stream_diff: TT[\"attention_layers\", \"pos\"] = torch.zeros(\n",
        "    model.cfg.n_layers, \n",
        "    tokens.shape[1], \n",
        "    device=\"cuda\", \n",
        "    dtype=torch.float32\n",
        "    )\n",
        "\n",
        "# For each attention layer (note there is really just one layer in this case!)\n",
        "for layer in range(model.cfg.n_layers):\n",
        "    # For each position in the prompt\n",
        "    for position in range(tokens.shape[1]):\n",
        "        # Run the model with patched residual components for just this layer &\n",
        "        # token\n",
        "        hook_fn = partial(patch_residual_component, pos=position, clean_cache=cache)\n",
        "        patched_logits: TT[\"batch\", \"pos\", \"vocab\"] = model.run_with_hooks(\n",
        "            corrupted_tokens, \n",
        "            fwd_hooks = [(utils.act_name(\"resid_pre\", layer), \n",
        "                hook_fn)], \n",
        "            return_type=\"logits\"\n",
        "        )\n",
        "        \n",
        "        # Note this would be broken if the final token is not the last in the\n",
        "        # prompt (due to end padding)\n",
        "        patched_logit_diff = logits_to_ave_logit_diff(patched_logits, answer_tokens)\n",
        "\n",
        "        patched_residual_stream_diff[layer, position] = normalize_patched_logit_diff(patched_logit_diff)\n",
        "        \n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Attention layers (not that useful as just 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yxMll_odZFPY",
        "outputId": "e833c29a-e6af-49a9-c3cd-4f50d7d09f4e"
      },
      "outputs": [],
      "source": [
        "prompt_position_labels = [f\"{tok}_{i}\" for i, tok in enumerate(model.to_str_tokens(tokens[0]))]\n",
        "imshow(patched_residual_stream_diff, x=prompt_position_labels, title=\"Logit Difference From Patched Residual Stream\", labels={\"x\":\"Position\", \"y\":\"Layer\"})"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Patching the residual for the attention layer, at the xc/xa tokens (there are 2),\n",
        "basically recovers 50% of performance for each one (i.e. 100% in total)."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "2qtlkrNPZFPY"
      },
      "source": [
        "### MLP layers (not that useful as just 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r_h13xamZFPY"
      },
      "source": [
        "We can apply exactly the same idea, but this time patching in attention or MLP layers. These are also residual components with identical shapes to the residual stream terms, so we can reuse the same hooks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YjEwuQquZFPY"
      },
      "outputs": [],
      "source": [
        "patched_attn_diff = torch.zeros(model.cfg.n_layers, tokens.shape[1], device=\"cuda\", dtype=torch.float32)\n",
        "patched_mlp_diff = torch.zeros(model.cfg.n_layers, tokens.shape[1], device=\"cuda\", dtype=torch.float32)\n",
        "for layer in range(model.cfg.n_layers):\n",
        "    for position in range(tokens.shape[1]):\n",
        "        hook_fn = partial(patch_residual_component, pos=position, clean_cache=cache)\n",
        "        patched_attn_logits = model.run_with_hooks(\n",
        "            corrupted_tokens, \n",
        "            fwd_hooks = [(utils.act_name(\"attn_out\", layer), \n",
        "                hook_fn)], \n",
        "            return_type=\"logits\"\n",
        "        )\n",
        "        patched_attn_logit_diff = logits_to_ave_logit_diff(patched_attn_logits, answer_tokens)\n",
        "        patched_mlp_logits = model.run_with_hooks(\n",
        "            corrupted_tokens, \n",
        "            fwd_hooks = [(utils.act_name(\"mlp_out\", layer), \n",
        "                hook_fn)], \n",
        "            return_type=\"logits\"\n",
        "        )\n",
        "        patched_mlp_logit_diff = logits_to_ave_logit_diff(patched_mlp_logits, answer_tokens)\n",
        "\n",
        "        patched_attn_diff[layer, position] = normalize_patched_logit_diff(patched_attn_logit_diff)\n",
        "        patched_mlp_diff[layer, position] = normalize_patched_logit_diff(patched_mlp_logit_diff)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZEcuX4UYZFPY",
        "outputId": "69c634f9-ea98-42f6-9a8c-6ad0bb7ff406"
      },
      "outputs": [],
      "source": [
        "imshow(patched_attn_diff, x=prompt_position_labels, title=\"Logit Difference From Patched Attention Layer\", labels={\"x\":\"Position\", \"y\":\"Layer\"})"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "YODI7zSKZFPY"
      },
      "source": [
        "The MLP layer already has the important info moved to the \"he\" token at the end,\n",
        "and this is the only one that matters for the final prediction so of course this\n",
        "will get 100%."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G6JmFzAgZFPY",
        "outputId": "8c2c217d-e25b-4b84-e313-337a5206ca64"
      },
      "outputs": [],
      "source": [
        "imshow(patched_mlp_diff, x=prompt_position_labels, title=\"Logit Difference From Patched MLP Layer\", labels={\"x\":\"Position\", \"y\":\"Layer\"})"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "C1Cz7ZiDZFPY"
      },
      "source": [
        "### Heads (not that useful)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cSx-KpXtZFPZ"
      },
      "outputs": [],
      "source": [
        "# We can refine the above analysis by patching in individual heads! This is\n",
        "# somewhat more annoying, because there are now three dimensions (head_index,\n",
        "# position and layer), so for now lets patch in a head's output across all\n",
        "# positions. \n",
        "\n",
        "# The easiest way to do this is to patch in the activation `z`, the \"mixed\n",
        "# value\" of the attention head. That is, the average of all previous values\n",
        "# weighted by the attention pattern, ie the activation that is then multiplied\n",
        "# by `W_O`, the output weights.  \n",
        "\n",
        "def patch_head_vector(\n",
        "    corrupted_head_vector: TT[\"batch\", \"pos\", \"head_index\", \"d_head\"],\n",
        "    hook, \n",
        "    head_index, \n",
        "    clean_cache):\n",
        "    corrupted_head_vector[:, :, head_index, :] = clean_cache[hook.name][:, :, head_index, :]\n",
        "    return corrupted_head_vector\n",
        "\n",
        "\n",
        "patched_head_z_diff = torch.zeros(model.cfg.n_layers, model.cfg.n_heads, device=\"cuda\", dtype=torch.float32)\n",
        "for layer in range(model.cfg.n_layers):\n",
        "    for head_index in range(model.cfg.n_heads):\n",
        "        hook_fn = partial(patch_head_vector, head_index=head_index, clean_cache=cache)\n",
        "        patched_logits = model.run_with_hooks(\n",
        "            corrupted_tokens, \n",
        "            fwd_hooks = [(utils.act_name(\"z\", layer, \"attn\"), \n",
        "                hook_fn)], \n",
        "            return_type=\"logits\"\n",
        "        )\n",
        "        patched_logit_diff = logits_to_ave_logit_diff(patched_logits, answer_tokens)\n",
        "\n",
        "        patched_head_z_diff[layer, head_index] = normalize_patched_logit_diff(patched_logit_diff)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4qnvYxMpZFPZ",
        "outputId": "dc6e816b-07d3-4794-e768-46bd827f4c25"
      },
      "outputs": [],
      "source": [
        "imshow(patched_head_z_diff, title=\"Logit Difference From Patched Head Output\", labels={\"x\":\"Head\", \"y\":\"Layer\"})"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "EwUKQpQ9ZFPZ"
      },
      "source": [
        "### Decomposing Heads"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Value patching (OV Circuit)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NTmQo8O_ZFPZ"
      },
      "outputs": [],
      "source": [
        "# First let's patch in the value vectors, to measure when figuring out what to\n",
        "# move is important. . This has the same shape as z ([batch, pos, head_index,\n",
        "# d_head]) so we can reuse the same hook. \n",
        "\n",
        "patched_head_v_diff = torch.zeros(model.cfg.n_layers, model.cfg.n_heads, device=\"cuda\", dtype=torch.float32)\n",
        "for layer in range(model.cfg.n_layers):\n",
        "    for head_index in range(model.cfg.n_heads):\n",
        "        hook_fn = partial(patch_head_vector, head_index=head_index, clean_cache=cache)\n",
        "        patched_logits = model.run_with_hooks(\n",
        "            corrupted_tokens, \n",
        "            fwd_hooks = [(utils.act_name(\"v\", layer, \"attn\"), \n",
        "                hook_fn)], \n",
        "            return_type=\"logits\"\n",
        "        )\n",
        "        patched_logit_diff = logits_to_ave_logit_diff(patched_logits, answer_tokens)\n",
        "\n",
        "        patched_head_v_diff[layer, head_index] = normalize_patched_logit_diff(patched_logit_diff)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MvjlvCfEZFPZ",
        "outputId": "1503394c-035b-4c4c-b68f-64620e3fc962"
      },
      "outputs": [],
      "source": [
        "imshow(patched_head_v_diff, title=\"Logit Difference From Patched Head Value\", labels={\"x\":\"Head\", \"y\":\"Layer\"})"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "SPIZ-u5FZFPZ"
      },
      "source": [
        "#### Value patching vs head output patching"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DA9rxGQ4ZFPZ",
        "outputId": "0b6028f0-ff6f-4a79-8f5e-8abc6459f5a9"
      },
      "outputs": [],
      "source": [
        "head_labels = [f\"L{l}H{h}\" for l in range(model.cfg.n_layers) for h in range(model.cfg.n_heads)]\n",
        "scatter(\n",
        "    x=utils.to_numpy(patched_head_v_diff.flatten()), \n",
        "    y=utils.to_numpy(patched_head_z_diff.flatten()), \n",
        "    xaxis=\"Value Patch\",\n",
        "    yaxis=\"Output Patch\",\n",
        "    caxis=\"Layer\",\n",
        "    hover_name = head_labels,\n",
        "    color=einops.repeat(np.arange(model.cfg.n_layers), \"layer -> (layer head)\", head=model.cfg.n_heads),\n",
        "    # range_x=(-0.5, 0.5),\n",
        "    # range_y=(-0.5, 0.5),\n",
        "    title=\"Scatter plot of output patching vs value patching\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "WzyOKXAvZFPZ"
      },
      "source": [
        "My impression is that this means the OV circuit is vital (how to move things)\n",
        "when looking at the difference between the corrupted and uncorrupted logits,\n",
        "which makes sense as the attention would be the same between both potential\n",
        "bigrams. "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Attention (QK) patching"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GeD6ipBHZFPZ"
      },
      "outputs": [],
      "source": [
        "def patch_head_pattern(\n",
        "    corrupted_head_pattern: TT[\"batch\", \"head_index\", \"query_pos\", \"d_head\"],\n",
        "    hook, \n",
        "    head_index, \n",
        "    clean_cache):\n",
        "    corrupted_head_pattern[:, head_index, :, :] = clean_cache[hook.name][:, head_index, :, :]\n",
        "    return corrupted_head_pattern\n",
        "\n",
        "patched_head_attn_diff = torch.zeros(model.cfg.n_layers, model.cfg.n_heads, device=\"cuda\", dtype=torch.float32)\n",
        "for layer in range(model.cfg.n_layers):\n",
        "    for head_index in range(model.cfg.n_heads):\n",
        "        hook_fn = partial(patch_head_pattern, head_index=head_index, clean_cache=cache)\n",
        "        patched_logits = model.run_with_hooks(\n",
        "            corrupted_tokens, \n",
        "            fwd_hooks = [(utils.act_name(\"attn\", layer, \"attn\"), \n",
        "                hook_fn)], \n",
        "            return_type=\"logits\"\n",
        "        )\n",
        "        patched_logit_diff = logits_to_ave_logit_diff(patched_logits, answer_tokens)\n",
        "\n",
        "        patched_head_attn_diff[layer, head_index] = normalize_patched_logit_diff(patched_logit_diff)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1u-yBWRgZFPZ",
        "outputId": "73bacaaa-13f6-4186-9589-8baae42f6312"
      },
      "outputs": [],
      "source": [
        "imshow(patched_head_attn_diff, title=\"Logit Difference From Patched Head Pattern\", labels={\"x\":\"Head\", \"y\":\"Layer\"})\n",
        "head_labels = [f\"L{l}H{h}\" for l in range(model.cfg.n_layers) for h in range(model.cfg.n_heads)]\n",
        "scatter(\n",
        "    x=utils.to_numpy(patched_head_attn_diff.flatten()), \n",
        "    y=utils.to_numpy(patched_head_z_diff.flatten()), \n",
        "    hover_name = head_labels,\n",
        "    xaxis=\"Attention Patch\",\n",
        "    yaxis=\"Output Patch\",\n",
        "    title=\"Scatter plot of output patching vs attention patching\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "aYDe_GEBZFPa"
      },
      "source": [
        "## Consolidating Understanding [TODO]"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### TODO\n",
        "\n",
        "- Look at specific MLP neurons (full_resid_decomp helps do direct logit\n",
        "  attribution) <HIGH>\n",
        "- Prove it is inductiony (e.g. remove he / make further apart e.g. 100s apart) <HIGHEST\n",
        "- If this fails, look for other induction promps (search data for prompts that\n",
        "  contain repeated bigrams, where 2nd occurance better than 1st & run same\n",
        "  tests).\n",
        "- Where unembed vectors have a high cosine similarity (e.g. all names), could\n",
        "  look for neurons that have a high cosine similarity with the difference\n",
        "  between these vectors"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Reference"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "a0N1z1YdZFPa"
      },
      "source": [
        "#### Visualizing Attention Patterns\n",
        "\n",
        "We can validate this by looking at the attention patterns of these heads! Let's take the top 10 heads by output patching (in absolute value) and split it into early, middle and late.\n",
        "\n",
        "We see that middle heads attend from the final token to the second subject, and late heads attend from the final token to the indirect object, which is completely consistent with the above speculation! But weirdly, while *one* early head attends from the second subject to its first copy, the other two mysteriously attend to the word *after* the first copy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U8CNAJLGZFPa",
        "outputId": "69da0ff2-920a-4a8f-a395-c2730b0f0c70"
      },
      "outputs": [],
      "source": [
        "top_k = 10\n",
        "top_heads_by_output_patch = torch.topk(patched_head_z_diff.abs().flatten(), k=top_k).indices\n",
        "first_mid_layer = 7\n",
        "first_late_layer = 9\n",
        "early_heads = top_heads_by_output_patch[top_heads_by_output_patch<model.cfg.n_heads * first_mid_layer]\n",
        "mid_heads = top_heads_by_output_patch[torch.logical_and(model.cfg.n_heads * first_mid_layer<=top_heads_by_output_patch, top_heads_by_output_patch<model.cfg.n_heads * first_late_layer)]\n",
        "late_heads = top_heads_by_output_patch[model.cfg.n_heads * first_late_layer<=top_heads_by_output_patch]\n",
        "visualize_attention_patterns(early_heads, title=f\"Top Early Heads\")\n",
        "visualize_attention_patterns(mid_heads, title=f\"Top Middle Heads\")\n",
        "visualize_attention_patterns(late_heads, title=f\"Top Late Heads\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "b2uWmLcNZFPa"
      },
      "source": [
        "#### Comparing to the Paper\n",
        "\n",
        "We can now refer to the (far, far more rigorous and detailed) analysis in the paper to compare our results! Here's the diagram they give of their results. \n",
        "\n",
        "![](https://pbs.twimg.com/media/FghGkTAWAAAmkhm.jpg)\n",
        "\n",
        "(Head 1.2 in their notation is L1H2 in my notation etc. And note - in the [latest version of the paper](https://arxiv.org/pdf/2211.00593.pdf) they add 9.0 as a backup name mover, and remove 11.3)\n",
        "\n",
        "The heads form three categories corresponding to the early, middle and late categhories we found and we did fairly well! Definitely not perfect, but with some fairly generic techniques and some a priori reasoning, we found the broad strokes of the circuit and what it looks like. We focused on the most important heads, so we didn't find all relevant heads in each category (especially not the heads in brackets, which are more minor), but this serves as a good base for doing more rigorous and involved analysis, especially for finding the *complete* circuit (ie all of the parts of the model which participate in this behaviour) rather than just a partial and suggestive circuit. Go check out [their paper](https://arxiv.org/abs/2211.00593) or [our interview](https://www.youtube.com/watch?v=gzwj0jWbvbo) to learn more about what they did and what they found!\n",
        "\n",
        "Breaking down their categories:\n",
        "\n",
        "* Early: The duplicate token heads, previous token heads and induction heads. These serve the purpose of detecting that the second subject is duplicated and which earlier name is the duplicate.\n",
        "    * We found a direct duplicate token head which behaves exactly as expected, L3H0. Heads L5H0 and L6H9 are induction heads, which explains why they don't attend directly to the earlier copy of John!\n",
        "    * Note that the duplicate token heads and induction heads do not compose with each other - both directly add to the S-Inhibition heads. The diagram is somewhat misleading.\n",
        "* Middle: They call these S-Inhibition heads - they copy the information about the duplicate token from the second subject to the to token, and their output is used to *inhibit* the attention paid from the name movers to the first subject copy. We found all these heads, and had a decent guess for what they did.\n",
        "    * In either case they attend to the second subject, so the patch that mattered was their value vectors!\n",
        "* Late: They call these name movers, and we found some of them. They attend from the final token to the indirect object name and copy that to the logits, using the S-Inhibition heads to inhibit attention to the first copy of the subject token.\n",
        "    * We did find their surprising result of *negative* name movers - name movers that inhibit the correct answer!\n",
        "    * They have an entire category of heads we missed called backup name movers - we'll get to these later.\n",
        "\n",
        "So, now, let's dig into the two anomalies we missed - induction heads and backup name mover heads"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "u_oJA60wZFPa"
      },
      "source": [
        "#### Early Heads are Induction Heads(?!)\n",
        "\n",
        "A really weird observation is that some of the early heads detecting duplicated tokens are induction heads, not just direct duplicate token heads. This is very weird! What's up with that? \n",
        "\n",
        "First off, what's an induction head? An induction head is an important type of attention head that can detect and continue repeated sequences. It is the second head in a two head induction circuit, which looks for previous copies of the current token and attends to the token *after* it, and then copies that to the current position and predicts that it will come next. They're enough of a big deal that [we wrote a whole paper on them](https://transformer-circuits.pub/2022/in-context-learning-and-induction-heads/index.html).\n",
        "\n",
        "![](https://pbs.twimg.com/media/FNWAzXjVEAEOGRe.jpg)\n",
        "\n",
        "Second, why is it surprising that they come up here? It's surprising because it feels like overkill. The model doesn't care about *what* token comes after the first copy of the subject, just that it's duplicated. And it already has simpler duplicate token heads. My best guess is that it just already had induction heads around and that, in addition to their main function, they *also* only activate on duplicated tokens. So it was useful to repurpose this existing machinery. \n",
        "\n",
        "This suggests that as we look for circuits in larger models life may get more and more complicated, as components in simpler circuits get repurposed and built upon. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yCvfTDYjZFPa"
      },
      "source": [
        "We can verify that these are induction heads by running the model on repeated text and plotting the heads."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hbMiUJJaZFPa",
        "outputId": "e97ea773-fb2b-4dfc-a15a-16813d0eed7f"
      },
      "outputs": [],
      "source": [
        "example_text = \"Research in mechanistic interpretability seeks to explain behaviors of machine learning models in terms of their internal components.\"\n",
        "example_repeated_text = example_text + example_text\n",
        "example_repeated_tokens = model.to_tokens(example_repeated_text, prepend_bos=True)\n",
        "example_repeated_logits, example_repeated_cache = model.run_with_cache(example_repeated_tokens)\n",
        "induction_head_labels = [81, 65]\n",
        "visualize_attention_patterns(induction_head_labels, example_repeated_cache, example_repeated_tokens, title=\"Induction Heads\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ml6_qnxZFPa"
      },
      "source": [
        "One implication of this is that it's useful to categories heads according to whether they occur in simpler circuits, so that as we look for more complex circuits we can easily look for them. This is easy to do here! An interesting fact about induction heads is that they work on a sequence of repeated random tokens - notable for being wildly off distribution from the natural language GPT-2 was trained on. Being able to predict a model's behaviour off distribution is a good mark of success for mechanistic interpretability! This is a good sanity check for whether a head is an induction head or not. \n",
        "\n",
        "We can characterise an induction head by just giving a sequence of random tokens repeated once, and measuring the average attention paid from the second copy of a token to the token after the first copy. At the same time, we can also measure the average attention paid from the second copy of a token to the first copy of the token, which is the attention that the induction head would pay if it were a duplicate token head, and the average attention paid to the previous token to find previous token heads.\n",
        "\n",
        "Note that this is a superficial study of whether something is an induction head - we totally ignore the question of whether it actually does boost the correct token or whether it composes with a single previous head and how. In particular, we sometimes get anti-induction heads which suppress the induction-y token (no clue why!), and this technique will find those too . But given the previous rigorous analysis, we can be pretty confident that this picks up on some true signal about induction heads.\n",
        "\n",
        "<details> <summary>Technical Implementation Details</summary> \n",
        "We can do this again by using hooks, this time just to access the attention patterns rather than to intervene on them. \n",
        "\n",
        "Our hook function acts on the attention pattern activation. This has the name \"blocks.{layer}.{layer_type}.hook_{activation_name}\" in general, here it's \"blocks.{layer}.attn.hook_attn\". And it has shape [batch, head_index, query_pos, token_pos]. Our hook function takes in the attention pattern activation, calculates the score for the relevant type of head, and write it to an external cache.\n",
        "\n",
        "We add in hooks using `model.run_with_hooks(tokens, fwd_hooks=[(names_filter, hook_fn)])` to temporarily add in the hooks and run the model, getting the resulting output. Previously names_filter was the name of the activation, but here it's a boolean function mapping activation names to whether we want to hook them or not. Here it's just whether the name ends with hook_attn. hook_fn must take in the two inputs activation (the activation tensor) and hook (the HookPoint object, which contains the name of the activation and some metadata such as the current layer).\n",
        "\n",
        "Internally our hooks use the function `tensor.diagonal`, this takes the diagonal between two dimensions, and allows an arbitrary offset - offset by 1 to get previous tokens, seq_len to get duplicate tokens (the distance to earlier copies) and seq_len-1 to get induction heads (the distance to the token *after* earlier copies). Different offsets give a different length of output tensor, and we can now just average to get a score in [0, 1] for each head\n",
        "</details>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FRuMBKm1ZFPa",
        "outputId": "8613f2ed-fd5e-42f9-caab-53b85e0bdbb9"
      },
      "outputs": [],
      "source": [
        "seq_len = 100\n",
        "batch_size = 2\n",
        "\n",
        "prev_token_scores = torch.zeros((model.cfg.n_layers, model.cfg.n_heads), device=\"cuda\")\n",
        "def prev_token_hook(pattern, hook):\n",
        "    layer = hook.layer()\n",
        "    diagonal = pattern.diagonal(offset=1, dim1=-1, dim2=-2)\n",
        "    # print(diagonal)\n",
        "    # print(pattern)\n",
        "    prev_token_scores[layer] = einops.reduce(diagonal, \"batch head_index diagonal -> head_index\", \"mean\")\n",
        "duplicate_token_scores = torch.zeros((model.cfg.n_layers, model.cfg.n_heads), device=\"cuda\")\n",
        "def duplicate_token_hook(pattern, hook):\n",
        "    layer = hook.layer()\n",
        "    diagonal = pattern.diagonal(offset=seq_len, dim1=-1, dim2=-2)\n",
        "    duplicate_token_scores[layer] = einops.reduce(diagonal, \"batch head_index diagonal -> head_index\", \"mean\")\n",
        "induction_scores = torch.zeros((model.cfg.n_layers, model.cfg.n_heads), device=\"cuda\")\n",
        "def induction_hook(pattern, hook):\n",
        "    layer = hook.layer()\n",
        "    diagonal = pattern.diagonal(offset=seq_len-1, dim1=-1, dim2=-2)\n",
        "    induction_scores[layer] = einops.reduce(diagonal, \"batch head_index diagonal -> head_index\", \"mean\")\n",
        "original_tokens = torch.randint(100, 20000, size=(batch_size, seq_len))\n",
        "repeated_tokens = einops.repeat(original_tokens, \"batch seq_len -> batch (2 seq_len)\").cuda()\n",
        "\n",
        "pattern_filter = lambda act_name: act_name.endswith(\"hook_attn\")\n",
        "loss = model.run_with_hooks(repeated_tokens, return_type=\"loss\", fwd_hooks=[(pattern_filter, prev_token_hook), (pattern_filter, duplicate_token_hook), (pattern_filter, induction_hook)])\n",
        "print(utils.get_corner(prev_token_scores))\n",
        "print(utils.get_corner(duplicate_token_scores))\n",
        "print(utils.get_corner(induction_scores))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c-jaFJQGZFPa"
      },
      "source": [
        "We can now plot the head scores, and instantly see that the relevant early heads are induction heads or duplicate token heads (though also that there's a lot of induction heads that are *not* use - I have no idea why!). "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iV_lapBJZFPa",
        "outputId": "a846a4c4-fcf4-4fd2-c2e6-69b1b37fd19c"
      },
      "outputs": [],
      "source": [
        "\n",
        "imshow(prev_token_scores, labels={\"x\":\"Head\", \"y\":\"Layer\"}, title=\"Previous Token Scores\")\n",
        "imshow(duplicate_token_scores, labels={\"x\":\"Head\", \"y\":\"Layer\"}, title=\"Duplicate Token Scores\")\n",
        "imshow(induction_scores, labels={\"x\":\"Head\", \"y\":\"Layer\"}, title=\"Induction Head Scores\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "APiPHIWeZFPa"
      },
      "source": [
        "The above suggests that it would be a useful bit of infrastructure to have a \"wiki\" for the heads of a model, giving their scores according to some metrics re head functions, like the ones we've seen here. EasyTransformer makes this easy to make, as just changing the name input to `EasyTransformer.from_pretrained` gives a different model but in the same architecture, so the same code should work. If you want to make this, I'd love to see it! \n",
        "\n",
        "As a proof of concept, [I made a mosaic of all induction heads across the 40 models then in EasyTransformer](https://www.neelnanda.io/mosaic)."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Nd3JoZ1CZFPa"
      },
      "source": [
        "#### Backup Name Mover Heads"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1snsb_cQZFPa"
      },
      "source": [
        "Another fascinating anomaly is that of the **backup name mover heads**. A standard technique to apply when interpreting model internals is ablations, or knock-out. If we run the model but intervene to set a specific head to zero, what happens? If the model is robust to this intervention, then naively we can be confident that the head is not doing anything important, and conversely if the model is much worse at the task this suggests that head was important. There are several conceptual flaws with this approach, making the evidence only suggestive, eg that the average output of the head may be far from zero and so the knockout may send it far from expected activations, breaking internals on *any* task. But it's still an easy technique to apply to give some data.\n",
        "\n",
        "But a wild finding in the paper is that models have **built in redundancy**. If we knock out one of the name movers, then there are some backup name movers in later layers that *change their behaviour* and do (some of) the job of the original name mover head. This means that naive knock-out will significantly underestimate the importance of the name movers.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uh88HUbEZFPb"
      },
      "source": [
        "Let's test this! Let's ablate the most important name mover (head L9H9) on just the final token using a custom ablation hook and then cache all new activations and compared performance. We focus on the final position because we want to specifically ablate the direct logit effect. When we do this, we see that naively, removing the top name mover should reduce the logit diff massively, from 3.55 to 0.57. **But actually, it only goes down to 2.99!**\n",
        "\n",
        "<details> <summary>Implementation Details</summary> \n",
        "Ablating heads is really easy in EasyTransformer! We can just define a hook on the z activation in the relevant attention layer (recall, z is the mixed values, and comes immediately before multiplying by the output weights $W_O$). z has a head_index axis, so we can set the component for the relevant head and for position -1 to zero, and return it. (Technically we could just edit in place without returning it, but by convention we always return an edited activation). \n",
        "\n",
        "We now want to compare all internal activations with a hook, which is hard to do with the nice `run_with_hooks` API. So we can directly access the hook on the z activation with `model.blocks[layer].attn.hook_z` and call its `add_hook` method. This adds in the hook to the *global state* of the model. We can now use run_with_cache, and don't need to care about the global state, because run_with_cache internally adds a bunch of caching hooks, and then removes all hooks after the run, *including* the previously added ablation hook. This can be disabled with the reset_hooks_end flag, but here it's useful! \n",
        "</details>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ICBvx7yJZFPb",
        "outputId": "3378a0d0-ff46-448a-8d23-4282739553a9"
      },
      "outputs": [],
      "source": [
        "top_name_mover = per_head_logit_diffs.flatten().argmax().item()\n",
        "top_name_mover_layer = top_name_mover//model.cfg.n_heads\n",
        "top_name_mover_head = top_name_mover % model.cfg.n_heads\n",
        "print(f\"Top Name Mover to ablate: L{top_name_mover_layer}H{top_name_mover_head}\")\n",
        "def ablate_top_head_hook(z: TT[\"batch\", \"pos\", \"head_index\", \"d_head\"], hook):\n",
        "    z[:, -1, top_name_mover_head, :] = 0\n",
        "    return z\n",
        "# Adds a hook into global model state\n",
        "model.blocks[top_name_mover_layer].attn.hook_z.add_hook(ablate_top_head_hook)\n",
        "# Runs the model, temporarily adds caching hooks and then removes *all* hooks after running, including the ablation hook.\n",
        "ablated_logits, ablated_cache = model.run_with_cache(tokens)\n",
        "print(f\"Original logit diff: {original_average_logit_diff}\")\n",
        "print(f\"Post ablation logit diff: {logits_to_ave_logit_diff(ablated_logits, answer_tokens).item()}\")\n",
        "print(f\"Direct Logit Attribution of top name mover head: {per_head_logit_diffs.flatten()[top_name_mover].item()}\")\n",
        "print(f\"Naive prediction of post ablation logit diff: {original_average_logit_diff - per_head_logit_diffs.flatten()[top_name_mover].item()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A-wsN2nhZFPb"
      },
      "source": [
        "So what's up with this? As before, we can look at the direct logit attribution of each head to see what's going on. It's easiest to interpret if plotted as a scatter plot against the initial per head logit difference.\n",
        "\n",
        "And we can see a *really* big difference in a few heads! (Hover to see labels) In particular the negative name mover L10H7 decreases its negative effect a lot, adding +1 to the logit diff, and the backup name mover L10H10 adjusts its effect to be more positive, adding +0.8 to the logit diff (with several other marginal changes). (And obviously the ablated head has gone down to zero!)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-g_PqYfsZFPb",
        "outputId": "8f6c8371-c7b4-41f7-b770-df15c30f8aa9"
      },
      "outputs": [],
      "source": [
        "per_head_ablated_residual, labels = ablated_cache.stack_head_results(layer=-1, pos_slice=-1, return_labels=True)\n",
        "per_head_ablated_logit_diffs = residual_stack_to_logit_diff(per_head_ablated_residual, ablated_cache)\n",
        "per_head_ablated_logit_diffs = per_head_ablated_logit_diffs.reshape(model.cfg.n_layers, model.cfg.n_heads)\n",
        "imshow(per_head_ablated_logit_diffs, labels={\"x\":\"Head\", \"y\":\"Layer\"})\n",
        "scatter(y=per_head_logit_diffs.flatten(), x=per_head_ablated_logit_diffs.flatten(), hover_name=head_labels, range_x=(-3, 3), range_y=(-3, 3), xaxis=\"Ablated\", yaxis=\"Original\", title=\"Original vs Post-Ablation Direct Logit Attribution of Heads\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pe9SaGUQZFPb"
      },
      "source": [
        "One natural hypothesis is that this is because the final LayerNorm scaling has changed, which can scale up or down the final residual stream. This is slightly true, and we can see that the typical head is a bit off from the x=y line. But the average LN scaling ratio is 1.04, and this should uniformly change *all* heads by the same factor, so this can't be sufficient"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2tiWDGt7ZFPb",
        "outputId": "cd28ed36-e3bb-48bf-bc40-4d0deb953635"
      },
      "outputs": [],
      "source": [
        "print(\"Average LN scaling ratio:\", (cache[\"ln_final.hook_scale\"][:, -1]/ablated_cache[\"ln_final.hook_scale\"][:, -1]).mean().item())\n",
        "print(\"Ablation LN scale\", ablated_cache[\"ln_final.hook_scale\"][:, -1])\n",
        "print(\"Original LN scale\", cache[\"ln_final.hook_scale\"][:, -1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SEzMdFyrZFPb"
      },
      "source": [
        "**Exercise to the reader:** Can you finish off this analysis? What's going on here? Why are the backup name movers changing their behaviour? Why is one negative name mover becoming significantly less important?"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.5"
    },
    "vscode": {
      "interpreter": {
        "hash": "daf231521c125af094d5066265592d6b64cf3d2c4d3459f02d2b6825d3ed03e7"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
