{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IN_COLAB = False\n",
    "print(\"Running as a Jupyter notebook - intended for development only!\")\n",
    "from IPython import get_ipython\n",
    "\n",
    "ipython = get_ipython()\n",
    "# Code to automatically update the EasyTransformer code as its edited without restarting the kernel\n",
    "ipython.magic(\"load_ext autoreload\")\n",
    "ipython.magic(\"autoreload 2\")\n",
    "\n",
    "# Import stuff\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import einops\n",
    "from fancy_einsum import einsum\n",
    "import tqdm.notebook as tqdm\n",
    "import random\n",
    "from pathlib import Path\n",
    "import plotly.express as px\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from torchtyping import TensorType as TT\n",
    "from typing import List, Union, Optional, Tuple\n",
    "from functools import partial\n",
    "import copy\n",
    "\n",
    "import itertools\n",
    "from transformers import AutoModelForCausalLM, AutoConfig, AutoTokenizer\n",
    "import dataclasses\n",
    "import datasets\n",
    "from IPython.display import HTML\n",
    "from torchtyping import TensorType\n",
    "\n",
    "import circuitsvis\n",
    "from circuitsvis import attention\n",
    "\n",
    "import easy_transformer\n",
    "import easy_transformer.utils as utils\n",
    "from easy_transformer.hook_points import (\n",
    "    HookedRootModule,\n",
    "    HookPoint,\n",
    ")  # Hooking utilities\n",
    "from easy_transformer import EasyTransformer, EasyTransformerConfig, FactoredMatrix, ActivationCache\n",
    "\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "torch.set_grad_enabled(False)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = EasyTransformer.from_pretrained(\n",
    "    \"NeelNanda/SoLU_1L512W_C4_Code\",\n",
    "    center_unembed=True,\n",
    "    center_writing_weights=True,\n",
    "    fold_ln=True,\n",
    "    refactor_factored_attn_matrices=True,\n",
    "    device=\"cpu\"\n",
    ")\n",
    "\n",
    "attn_model = EasyTransformer.from_pretrained(\n",
    "    \"NeelNanda/Attn_Only_1L512W_C4_Code\",\n",
    "    center_unembed=True,\n",
    "    center_writing_weights=True,\n",
    "    fold_ln=True,\n",
    "    refactor_factored_attn_matrices=True,\n",
    "    device=\"cpu\"\n",
    "    )\n",
    "\n",
    "tokenizer = model.tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = []\n",
    "# 48261\n",
    "for tokenIndex in tqdm(range(48261)):\n",
    "    # Get the token\n",
    "    token_idx = tokenizer.decode(tokenIndex)\n",
    "    \n",
    "    # Initialise the token details\n",
    "    details = {\n",
    "        \"idx\": tokenIndex,\n",
    "        \"token\": token_idx,\n",
    "    }\n",
    "    \n",
    "    # Get the top k logits & corresponding tokens\n",
    "    logits = model(token_idx)[0][1] * -1\n",
    "    log_probs = F.log_softmax(logits, dim=-1)\n",
    "    _, topKTokens = torch.topk(log_probs, 10, largest=False)\n",
    "    \n",
    "    for idx, token_idx in enumerate(topKTokens):\n",
    "        token_idx = token_idx.item()\n",
    "        token = tokenizer.decode(token_idx)\n",
    "        log_prob = log_probs[token_idx]\n",
    "        \n",
    "        # Add to the details\n",
    "        details[f\"top_{idx}_idx\"] = token_idx\n",
    "        details[f\"top_{idx}_token\"] = token\n",
    "        details[f\"top_{idx}_log_prob\"] = log_prob.item()\n",
    "        details[f\"top_{idx}_prob\"] = torch.exp(log_prob).item()\n",
    "    \n",
    "    # Add to tokens list\n",
    "    tokens.append(details)\n",
    "    \n",
    "# Convert to a dataframe\n",
    "bigrams = pd.DataFrame(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigrams.sort_values(\"top_0_log_prob\", ascending=True).head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of prompts (as a list of token indices)\n",
    "prompts: List[List[int]] = []\n",
    "# List of answers, in the format (correct, incorrect)\n",
    "answers: List[Tuple[int, int]] = []\n",
    "\n",
    "for idx, row in bigrams.iterrows():\n",
    "    tokenIdx = int(row[\"idx\"])\n",
    "    most_likely = int(row[\"top_0_idx\"])\n",
    "    second_likely = int(row[\"top_1_idx\"])\n",
    "    prompt = [tokenIdx, second_likely, 510, tokenIdx]\n",
    "    answer = (second_likely, most_likely)\n",
    "    prompts.append(prompt)\n",
    "    answers.append(answer)\n",
    "    \n",
    "prompts = torch.tensor(prompts)\n",
    "answers = torch.tensor(answers)\n",
    "\n",
    "prompts.shape, answers.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "prompts = []\n",
    "answers_correct = []\n",
    "answers_incorrect = []\n",
    "\n",
    "for idx, row in bigrams.iterrows():\n",
    "    # Get the token & most likely next tokens\n",
    "    token = str(row[\"token\"])\n",
    "    most_likely = str(row[\"top_0_token\"])\n",
    "    less_likely = str(row[\"top_1_token\"]) # Can change to 1-9 \n",
    "    \n",
    "    # Skip if any have spaces\n",
    "    if \" \" in token or \" \" in most_likely or \" \" in less_likely:\n",
    "        continue\n",
    "    \n",
    "    # Skip if token string contains non-letters\n",
    "    if not token.isalpha() or not most_likely.isalpha() or not less_likely.isalpha():\n",
    "        continue\n",
    "        \n",
    "    # Create the prompt\n",
    "    prompts.append(f\".{token}{less_likely} .{token}{less_likely} .{token}\")\n",
    "    answers_correct.append(less_likely)\n",
    "    answers_incorrect.append(most_likely)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame([prompts, answers_correct, answers_incorrect]).T\n",
    "\n",
    "# Set all column types as strings\n",
    "df = df.astype(str)\n",
    "df.columns = [\"prompt\", \"answer_correct\", \"answer_incorrect\"]\n",
    "\n",
    "# Count any answer_correct that are not strings\n",
    "not_strings = df[\"answer_correct\"].apply(lambda x: not isinstance(x, str))\n",
    "\n",
    "df.to_csv(\"bigram_prompts.csv\", index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attn vs SoLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2853it [00:53, 53.37it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "count    2853.000000\n",
       "mean        0.000044\n",
       "std         0.000184\n",
       "min        -0.000205\n",
       "25%        -0.000023\n",
       "50%         0.000003\n",
       "75%         0.000052\n",
       "max         0.000879\n",
       "dtype: float64"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diffs = []\n",
    "\n",
    "for idx, prompt in tqdm(enumerate(prompts)):\n",
    "    answer = answers_correct[idx]\n",
    "    answer_token = model.to_single_token(answer[0])\n",
    "    \n",
    "    solu_logits = model(prompt)\n",
    "    attn_logits = attn_model(prompt)\n",
    "    \n",
    "    solu_last_logits = solu_logits[0][1]\n",
    "    attn_last_logits = attn_logits[0][1]\n",
    "    \n",
    "    solu_probs = F.softmax(solu_last_logits, dim=-1)\n",
    "    attn_probs = F.softmax(attn_last_logits, dim=-1)\n",
    "    \n",
    "    solu_correct_prob = solu_probs[answer_token]\n",
    "    attn_correct_prob = attn_probs[answer_token]\n",
    "    \n",
    "    diff = solu_correct_prob - attn_correct_prob\n",
    "    \n",
    "    diffs.append(diff.item())\n",
    "\n",
    "    \n",
    "diffs_series = pd.Series(diffs)\n",
    "\n",
    "# Show summary statistics\n",
    "diffs_series.describe()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "daf231521c125af094d5066265592d6b64cf3d2c4d3459f02d2b6825d3ed03e7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
