{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "031e47c8-3024-4865-8fb3-6429af0f8bbf",
   "metadata": {},
   "source": [
    "# SoLU Circuits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cb718ae-d144-4c98-b3dc-0a230f8a46cc",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "08a22465-7f12-4158-bd35-721465c374e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.solu_utils import model, tokenizer\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from IPython.display import clear_output\n",
    "\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0ba7396-663b-4401-aaec-ef7762b5ea73",
   "metadata": {},
   "source": [
    "## Find interesting activations\n",
    "\n",
    "A 1-layer model without an MLP can't do much more than skip trigrams. Whilst the MLP layer added may improve this a little, the prompts will still need to have quite simple answers.\n",
    "\n",
    "In this case we'll look for the ability of the model to close HTML tags. As an simple overview of how HTML tags work, whenever a tag is used (e.g. `<b>` for bold) it must be closed when you no longer want it to apply (e.g. `<b>bold text</b> normal text`).\n",
    "\n",
    "Note that `</` is a single token - so we can't use `<` as the last token and expect to see `\\`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3402d3fe-e412-4fcd-9bf8-a58c69467d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_next_token(prompt: str) -> str:\n",
    "    \"\"\"Run a forward pass to get the next token\"\"\"\n",
    "    logits = model(prompt)[0]\n",
    "    log_probabilities = F.log_softmax(logits, dim=-1)\n",
    "    predictions = torch.argmax(log_probabilities, 2)\n",
    "    next_token = [model.tokenizer.decode(t) for t in predictions.squeeze()][-1]\n",
    "    return next_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8c8b608e-250b-499a-802b-887775d0bda2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<h1>Title</h\n",
      "<b>Some bold text</b>\n",
      "<p>An interesting paragraph</p>\n",
      "<table><tr><th>Model name</th\n"
     ]
    }
   ],
   "source": [
    "# Example prompts to run through the model\n",
    "prompts = [\n",
    "    \"<h1>Title\",\n",
    "    \"<b>Some bold text</\",\n",
    "    \"<p>An interesting paragraph</\",\n",
    "    \"<table><tr><th>Model name\"\n",
    "]\n",
    "\n",
    "# Run each prompt (with a few tokens appended by the model)\n",
    "for prompt in prompts:\n",
    "    result = prompt\n",
    "    \n",
    "    additional_tokens = 2\n",
    "    for i in range(additional_tokens):\n",
    "        next_token = get_next_token(result)\n",
    "        result = result + next_token\n",
    "        \n",
    "    print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
