{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "031e47c8-3024-4865-8fb3-6429af0f8bbf",
   "metadata": {},
   "source": [
    "# SoLU Circuits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cb718ae-d144-4c98-b3dc-0a230f8a46cc",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "08a22465-7f12-4158-bd35-721465c374e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import copy\n",
    "import gc\n",
    "import itertools\n",
    "import json\n",
    "import math\n",
    "import os\n",
    "import pickle\n",
    "import random\n",
    "import sys\n",
    "import time\n",
    "from functools import partial\n",
    "from os import path\n",
    "from pathlib import Path\n",
    "from pprint import pprint\n",
    "\n",
    "import datasets\n",
    "import einops\n",
    "import gdown\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import plotly.io as pio\n",
    "import pysvelte\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import tqdm.auto as tqdm\n",
    "import tqdm.notebook as tqdm\n",
    "import transformers\n",
    "import wandb\n",
    "from datasets import load_dataset\n",
    "from easy_transformer.EasyTransformerConfig import EasyTransformerConfig\n",
    "from easy_transformer.EasyTransformer import EasyTransformer, Embed, Unembed, PosEmbed, LayerNorm, Attention, MLP, TransformerBlock\n",
    "from easy_transformer.hook_points import HookedRootModule, HookPoint\n",
    "from IPython.display import clear_output\n",
    "from rich import print\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoConfig, AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# Fix for pysvelte import bug\n",
    "sys.path.append('/home/user/.local/lib/python3.9/site-packages/pysvelte')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96fb5832-1076-4dea-aad6-cd12a30a2ea0",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a8bc566-a58d-4395-b198-a3c42221ac2c",
   "metadata": {},
   "source": [
    "### Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "89b92a3b-4988-42f0-a03f-31af959d7856",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EasyTransformerConfig settings\n",
    "cfg = {\n",
    "    'd_model': 1024,\n",
    "    'd_head': 64,\n",
    "    'n_layers': 1,\n",
    "    'n_ctx': 1024,\n",
    "    'd_vocab': 50278,\n",
    "    'use_attn_result': False,\n",
    "    'act_fn': 'solu_ln',\n",
    "    'eps': 1e-5,\n",
    "    # The trained model used LN everywhere, except for RMS just before \n",
    "    # the final unembedding. We switch to LNPre (folding in the weights/biases\n",
    "    # to the next weights), and then manually override the final RMS normalization\n",
    "    # to be RMSPre in the code below. See the 'Fold in weights and biases' section\n",
    "    # for more details.\n",
    "    'normalization_type': 'LNPre',\n",
    "    \"model_name\": \"SoLU\"\n",
    "}\n",
    "\n",
    "# Calculated settings\n",
    "cfg['n_heads'] = cfg['d_model']//cfg['d_head']\n",
    "cfg['d_mlp'] = 4 * cfg['d_model']\n",
    "\n",
    "# Custom settings not supported by EasyTransformer directly\n",
    "custom_cfg = {\n",
    "    'model_checkpoint_name': 'SoLU_1L_1024W_final_checkpoint.pth',\n",
    "    'device': 'cuda',\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b452093-651d-4ffd-900e-8bfc06382072",
   "metadata": {},
   "source": [
    "### Model Setup\n",
    "\n",
    "This uses the `EasyTransformer` components, where possible (as they can be configured identically to the code that was used for training)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f013bc5f-848b-4949-8fc5-32694887e777",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Transformer(\n",
       "  (embed): Embed()\n",
       "  (hook_embed): HookPoint()\n",
       "  (pos_embed): PosEmbed()\n",
       "  (hook_pos_embed): HookPoint()\n",
       "  (blocks): ModuleList(\n",
       "    (0): TransformerBlock(\n",
       "      (ln1): LayerNormPre(\n",
       "        (hook_scale): HookPoint()\n",
       "        (hook_normalized): HookPoint()\n",
       "      )\n",
       "      (ln2): LayerNormPre(\n",
       "        (hook_scale): HookPoint()\n",
       "        (hook_normalized): HookPoint()\n",
       "      )\n",
       "      (attn): Attention(\n",
       "        (hook_k): HookPoint()\n",
       "        (hook_q): HookPoint()\n",
       "        (hook_v): HookPoint()\n",
       "        (hook_z): HookPoint()\n",
       "        (hook_attn_scores): HookPoint()\n",
       "        (hook_attn): HookPoint()\n",
       "        (hook_result): HookPoint()\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (hook_pre): HookPoint()\n",
       "        (hook_post): HookPoint()\n",
       "        (hook_post_ln): HookPoint()\n",
       "        (ln): LayerNorm(\n",
       "          (hook_scale): HookPoint()\n",
       "          (hook_normalized): HookPoint()\n",
       "        )\n",
       "      )\n",
       "      (hook_attn_out): HookPoint()\n",
       "      (hook_mlp_out): HookPoint()\n",
       "      (hook_resid_pre): HookPoint()\n",
       "      (hook_resid_mid): HookPoint()\n",
       "      (hook_resid_post): HookPoint()\n",
       "    )\n",
       "  )\n",
       "  (ln_final): RMSNormPre(\n",
       "    (hook_scale): HookPoint()\n",
       "  )\n",
       "  (unembed): Unembed()\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class RMSNormPre(nn.Module):\n",
    "    \"\"\"RMS Pre Normalization\n",
    "    \n",
    "    This is RMS Normalization without the multiplation by a weights term, \n",
    "    as that has been folded into the next weights instead.\"\"\"\n",
    "\n",
    "    def __init__(self, cfg, length):\n",
    "        super().__init__()\n",
    "        self.eps = cfg.eps\n",
    "        self.length = length\n",
    "        # self.w = nn.Parameter(torch.ones(length)) # Folded\n",
    "\n",
    "        # Adds a hook point for the normalization scale factor\n",
    "        self.hook_scale = HookPoint()  # [batch, pos]\n",
    "\n",
    "    def forward(self, x):\n",
    "        scale = self.hook_scale((x.pow(2).mean(-1, keepdim=True) +\n",
    "                                 self.eps).sqrt())  # [batch, pos, 1]\n",
    "        out = (x / scale) # * self.w # (folded)\n",
    "        return out\n",
    "\n",
    "    \n",
    "class Transformer(EasyTransformer):\n",
    "    \"\"\"Transformer\n",
    "    \n",
    "    The training run that we'll be loading had a few modifications from the\n",
    "    standard `EasyTransformer`, so we extend it and add these in here.\n",
    "    \"\"\"\n",
    "    def __init__(self, cfg: EasyTransformerConfig):\n",
    "        super().__init__(\"custom\", cfg=cfg)\n",
    "        \n",
    "        # Custom tokenizer setup (different pad token) from trained model\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained('EleutherAI/gpt-neox-20b')\n",
    "        pad_token = '<PAD>'\n",
    "        self.tokenizer.add_special_tokens({'pad_token': pad_token})\n",
    "        \n",
    "        # Custom final layer norm (trained model used RMS Norm, and we've \n",
    "        # folded the weights out of this)\n",
    "        self.ln_final = RMSNormPre(self.cfg, self.cfg.d_model)\n",
    "   \n",
    "    def to_tokens(self, text):\n",
    "        return self.tokenizer(self.tokenizer.bos_token+text, return_tensors='pt')['input_ids'].to(custom_cfg['device'])\n",
    "\n",
    "    \n",
    "# Create the model\n",
    "model = Transformer(EasyTransformerConfig.from_dict(cfg))\n",
    "model.to(custom_cfg['device'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3708f5ca-51d5-4366-80c7-db4aa91fb880",
   "metadata": {},
   "source": [
    "### Load from the checkpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0797c77-e691-410b-ab95-45eba3faff3c",
   "metadata": {},
   "source": [
    "### Download Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "99cb296e-919e-4273-b375-89184849d5fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checkpoint provided by Neel Nanda\n",
    "checkpoint_url = \"https://drive.google.com/file/d/16bqEZg9Oq0WT2xOcNS1HJkmR7qB2G14o/view\"\n",
    "\n",
    "# Create the directory if it doesn't exist\n",
    "checkpoint_dir = \"/tmp/checkpoints\"\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "\n",
    "# Download the checkpoint if it doesn't exist\n",
    "checkpoint_file = path.join(checkpoint_dir, custom_cfg['model_checkpoint_name'])\n",
    "if not path.exists(checkpoint_file):\n",
    "    gdown.download(checkpoint_url, checkpoint_file, quiet=False, fuzzy=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a163807-3abb-4483-90eb-927908430ff4",
   "metadata": {},
   "source": [
    "### Fold in weights and biases\n",
    "\n",
    "We fold the `LayerNorm` weights and biases in to the weights after them, for simplicty, as per [A Mathematical Framework for Transformer Circuits](https://transformer-circuits.pub/2021/framework/index.html#model-simplifications)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7d8dd564-de63-400b-a51b-a9cc0ee99a1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">odict_keys</span><span style=\"font-weight: bold\">([</span><span style=\"color: #008000; text-decoration-color: #008000\">'embed.W_E'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'pos_embed.W_pos'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'norm.w'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'blocks.0.norm1.w'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'blocks.0.norm1.b'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'blocks.0.norm2.w'</span>, \n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'blocks.0.norm2.b'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'blocks.0.attn.W_Q'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'blocks.0.attn.b_Q'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'blocks.0.attn.W_K'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'blocks.0.attn.b_K'</span>, \n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'blocks.0.attn.W_V'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'blocks.0.attn.b_V'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'blocks.0.attn.W_O'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'blocks.0.attn.b_O'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'blocks.0.attn.mask'</span>, \n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'blocks.0.attn.IGNORE'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'blocks.0.mlp.W_in'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'blocks.0.mlp.b_in'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'blocks.0.mlp.W_out'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'blocks.0.mlp.b_out'</span>, \n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'blocks.0.mlp.ln.w'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'blocks.0.mlp.ln.b'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'unembed.W_U'</span><span style=\"font-weight: bold\">])</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35modict_keys\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[32m'embed.W_E'\u001b[0m, \u001b[32m'pos_embed.W_pos'\u001b[0m, \u001b[32m'norm.w'\u001b[0m, \u001b[32m'blocks.0.norm1.w'\u001b[0m, \u001b[32m'blocks.0.norm1.b'\u001b[0m, \u001b[32m'blocks.0.norm2.w'\u001b[0m, \n",
       "\u001b[32m'blocks.0.norm2.b'\u001b[0m, \u001b[32m'blocks.0.attn.W_Q'\u001b[0m, \u001b[32m'blocks.0.attn.b_Q'\u001b[0m, \u001b[32m'blocks.0.attn.W_K'\u001b[0m, \u001b[32m'blocks.0.attn.b_K'\u001b[0m, \n",
       "\u001b[32m'blocks.0.attn.W_V'\u001b[0m, \u001b[32m'blocks.0.attn.b_V'\u001b[0m, \u001b[32m'blocks.0.attn.W_O'\u001b[0m, \u001b[32m'blocks.0.attn.b_O'\u001b[0m, \u001b[32m'blocks.0.attn.mask'\u001b[0m, \n",
       "\u001b[32m'blocks.0.attn.IGNORE'\u001b[0m, \u001b[32m'blocks.0.mlp.W_in'\u001b[0m, \u001b[32m'blocks.0.mlp.b_in'\u001b[0m, \u001b[32m'blocks.0.mlp.W_out'\u001b[0m, \u001b[32m'blocks.0.mlp.b_out'\u001b[0m, \n",
       "\u001b[32m'blocks.0.mlp.ln.w'\u001b[0m, \u001b[32m'blocks.0.mlp.ln.b'\u001b[0m, \u001b[32m'unembed.W_U'\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the state dictionary from the checkpoint\n",
    "sd = torch.load(checkpoint_file)\n",
    "print(sd.keys())\n",
    "\n",
    "# Fold in layer normalization weights & biases, for each layer (just one in the toy example)\n",
    "for layer in range(cfg['n_layers']):\n",
    "    # Pre-attention layer norm weights -> Query/Key/Value weights\n",
    "    pre_ln_w = sd[f\"blocks.{layer}.norm1.w\"]\n",
    "    W_Q_old = sd[f\"blocks.{layer}.attn.W_Q\"]\n",
    "    W_K_old = sd[f\"blocks.{layer}.attn.W_K\"]\n",
    "    W_V_old = sd[f\"blocks.{layer}.attn.W_V\"]\n",
    "    sd[f\"blocks.{layer}.attn.W_Q\"] = W_Q_old * pre_ln_w\n",
    "    sd[f\"blocks.{layer}.attn.W_K\"] = W_K_old * pre_ln_w\n",
    "    sd[f\"blocks.{layer}.attn.W_V\"] = W_V_old * pre_ln_w\n",
    "    \n",
    "    # Pre-attention layer norm biases -> Query/Key/Value biases\n",
    "    pre_ln_b = sd[f\"blocks.{layer}.norm1.b\"]\n",
    "    sd[f\"blocks.{layer}.attn.b_Q\"] = W_Q_old @ pre_ln_b + sd[f\"blocks.{layer}.attn.b_Q\"]\n",
    "    sd[f\"blocks.{layer}.attn.b_K\"] = W_K_old @ pre_ln_b + sd[f\"blocks.{layer}.attn.b_K\"]\n",
    "    sd[f\"blocks.{layer}.attn.b_V\"] = W_V_old @ pre_ln_b + sd[f\"blocks.{layer}.attn.b_V\"]\n",
    "    \n",
    "    # Post-attention layer weights/biases -> MLP weights/biases\n",
    "    W_in_old = sd[f\"blocks.{layer}.mlp.W_in\"]\n",
    "    sd[f\"blocks.{layer}.mlp.W_in\"] = W_in_old * sd[f\"blocks.{layer}.norm2.w\"]\n",
    "    sd[f\"blocks.{layer}.mlp.b_in\"] = W_in_old @ sd[f\"blocks.{layer}.norm2.b\"] \\\n",
    "                                        + sd[f\"blocks.{layer}.mlp.b_in\"]\n",
    "    \n",
    "    # Delete the weights/biases that are no longer used (as they're folded in)\n",
    "    del sd[f\"blocks.{layer}.norm1.w\"]\n",
    "    del sd[f\"blocks.{layer}.norm1.b\"]\n",
    "    del sd[f\"blocks.{layer}.norm2.w\"]\n",
    "    del sd[f\"blocks.{layer}.norm2.b\"]\n",
    "\n",
    "# Fold the post-blocks (pre-unembed) RMS norm weights -> unembed weights\n",
    "sd[\"unembed.W_U\"] *= sd[\"norm.w\"]\n",
    "del sd[\"norm.w\"] # Delete as no longer used (folded in)\n",
    "    \n",
    "# EasyTransformer has an additional bias term for the unembedding, so we simply set it to zero.\n",
    "sd[\"unembed.b_U\"] = torch.zeros(cfg['d_vocab'])\n",
    "\n",
    "# Load the state dict into the model\n",
    "model.load_state_dict(sd)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c046113-3580-4e37-9dcf-2a836a6f67dc",
   "metadata": {},
   "source": [
    "### Load the checkpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0ba7396-663b-4401-aaec-ef7762b5ea73",
   "metadata": {},
   "source": [
    "## Find interesting activations\n",
    "\n",
    "A 1-layer model without an MLP can't do much more than skip trigrams. Whilst the MLP layer added may improve this a little, the prompts will still need to have quite simple answers.\n",
    "\n",
    "In this case we'll look for the ability of the model to close HTML tags. As an simple overview of how HTML tags work, whenever a tag is used (e.g. `<b>` for bold) it must be closed when you no longer want it to apply (e.g. `<b>bold text</b> normal text`).\n",
    "\n",
    "Note that `</` is a single token - so we can't use `<` as the last token and expect to see `\\`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3402d3fe-e412-4fcd-9bf8-a58c69467d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_next_token(prompt: str) -> str:\n",
    "    \"\"\"Run a forward pass to get the next token\"\"\"\n",
    "    logits = model(prompt, \"logits\")\n",
    "    log_probabilities = F.log_softmax(logits, dim=-1)\n",
    "    predictions = torch.argmax(log_probabilities, 2)\n",
    "    next_token = [model.tokenizer.decode(t) for t in predictions.squeeze()][-1]\n",
    "    return next_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8c8b608e-250b-499a-802b-887775d0bda2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">h1</span><span style=\"font-weight: bold\">&gt;</span>Title&lt;<span style=\"color: #800080; text-decoration-color: #800080\">/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">h</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m<\u001b[0m\u001b[1;95mh1\u001b[0m\u001b[1m>\u001b[0mTitle<\u001b[35m/\u001b[0m\u001b[95mh\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">b</span><span style=\"font-weight: bold\">&gt;</span>Some bold text<span style=\"font-weight: bold\">&lt;</span><span style=\"color: #800080; text-decoration-color: #800080\">/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">b</span><span style=\"font-weight: bold\">&gt;</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m<\u001b[0m\u001b[1;95mb\u001b[0m\u001b[1m>\u001b[0mSome bold text\u001b[1m<\u001b[0m\u001b[35m/\u001b[0m\u001b[95mb\u001b[0m\u001b[1m>\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">p</span><span style=\"font-weight: bold\">&gt;</span>An interesting paragraph<span style=\"font-weight: bold\">&lt;</span><span style=\"color: #800080; text-decoration-color: #800080\">/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">p</span><span style=\"font-weight: bold\">&gt;</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m<\u001b[0m\u001b[1;95mp\u001b[0m\u001b[1m>\u001b[0mAn interesting paragraph\u001b[1m<\u001b[0m\u001b[35m/\u001b[0m\u001b[95mp\u001b[0m\u001b[1m>\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">table</span><span style=\"font-weight: bold\">&gt;&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">tr</span><span style=\"font-weight: bold\">&gt;&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">th</span><span style=\"font-weight: bold\">&gt;</span>Model name&lt;<span style=\"color: #800080; text-decoration-color: #800080\">/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">th</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m<\u001b[0m\u001b[1;95mtable\u001b[0m\u001b[1m>\u001b[0m\u001b[1m<\u001b[0m\u001b[1;95mtr\u001b[0m\u001b[1m>\u001b[0m\u001b[1m<\u001b[0m\u001b[1;95mth\u001b[0m\u001b[1m>\u001b[0mModel name<\u001b[35m/\u001b[0m\u001b[95mth\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Example prompts to run through the model\n",
    "prompts = [\n",
    "    \"<h1>Title\",\n",
    "    \"<b>Some bold text</\",\n",
    "    \"<p>An interesting paragraph</\",\n",
    "    \"<table><tr><th>Model name\"\n",
    "]\n",
    "\n",
    "# Run each prompt (with a few tokens appended by the model)\n",
    "for prompt in prompts:\n",
    "    result = prompt\n",
    "    \n",
    "    additional_tokens = 2\n",
    "    for i in range(additional_tokens):\n",
    "        next_token = get_next_token(result)\n",
    "        result = result + next_token\n",
    "        \n",
    "    print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "f9f85f796d01129d0dd105a088854619f454435301f6ffec2fea96ecbd9be4ac"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
